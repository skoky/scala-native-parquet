/**
 * Generated by Scrooge
 *   version: 4.16.0-SNAPSHOT
 *   rev: eb110c820bcd2734b26023f24d636bf6d37511b3
 *   built at: 20170607-185808
 */
package parquet.format

import com.twitter.io.Buf
import com.twitter.scrooge.{
  HasThriftStructCodec3,
  LazyTProtocol,
  TFieldBlob,
  ThriftException,
  ThriftStruct,
  ThriftStructCodec3,
  ThriftStructFieldInfo,
  ThriftStructMetaData,
  ThriftUtil
}
import org.apache.thrift.protocol._
import org.apache.thrift.transport.{TMemoryBuffer, TTransport, TIOStreamTransport}
import java.io.ByteArrayInputStream
import java.nio.ByteBuffer
import java.util.Arrays
import java.util.concurrent.atomic.AtomicInteger
import scala.collection.immutable.{Map => immutable$Map}
import scala.collection.mutable.Builder
import scala.collection.mutable.{
  ArrayBuffer => mutable$ArrayBuffer, Buffer => mutable$Buffer,
  HashMap => mutable$HashMap, HashSet => mutable$HashSet}
import scala.collection.{Map, Set}

/**
 * New page format alowing reading levels without decompressing the data
 * Repetition and definition levels are uncompressed
 * The remaining section containing the data is compressed if is_compressed is true
 **/
object DataPageHeaderV2 extends ThriftStructCodec3[DataPageHeaderV2] {
  val NoPassthroughFields: immutable$Map[Short, TFieldBlob] = immutable$Map.empty[Short, TFieldBlob]
  val Struct = new TStruct("DataPageHeaderV2")
  val NumValuesField = new TField("num_values", TType.I32, 1)
  val NumValuesFieldManifest = implicitly[Manifest[Int]]
  val NumNullsField = new TField("num_nulls", TType.I32, 2)
  val NumNullsFieldManifest = implicitly[Manifest[Int]]
  val NumRowsField = new TField("num_rows", TType.I32, 3)
  val NumRowsFieldManifest = implicitly[Manifest[Int]]
  val EncodingField = new TField("encoding", TType.ENUM, 4)
  val EncodingFieldI32 = new TField("encoding", TType.I32, 4)
  val EncodingFieldManifest = implicitly[Manifest[parquet.format.Encoding]]
  val DefinitionLevelsByteLengthField = new TField("definition_levels_byte_length", TType.I32, 5)
  val DefinitionLevelsByteLengthFieldManifest = implicitly[Manifest[Int]]
  val RepetitionLevelsByteLengthField = new TField("repetition_levels_byte_length", TType.I32, 6)
  val RepetitionLevelsByteLengthFieldManifest = implicitly[Manifest[Int]]
  val IsCompressedField = new TField("is_compressed", TType.BOOL, 7)
  val IsCompressedFieldManifest = implicitly[Manifest[Boolean]]
  val StatisticsField = new TField("statistics", TType.STRUCT, 8)
  val StatisticsFieldManifest = implicitly[Manifest[parquet.format.Statistics]]

  /**
   * Field information in declaration order.
   */
  lazy val fieldInfos: scala.List[ThriftStructFieldInfo] = scala.List[ThriftStructFieldInfo](
    new ThriftStructFieldInfo(
      NumValuesField,
      false,
      true,
      NumValuesFieldManifest,
      _root_.scala.None,
      _root_.scala.None,
      immutable$Map.empty[String, String],
      immutable$Map.empty[String, String],
      None
    ),
    new ThriftStructFieldInfo(
      NumNullsField,
      false,
      true,
      NumNullsFieldManifest,
      _root_.scala.None,
      _root_.scala.None,
      immutable$Map.empty[String, String],
      immutable$Map.empty[String, String],
      None
    ),
    new ThriftStructFieldInfo(
      NumRowsField,
      false,
      true,
      NumRowsFieldManifest,
      _root_.scala.None,
      _root_.scala.None,
      immutable$Map.empty[String, String],
      immutable$Map.empty[String, String],
      None
    ),
    new ThriftStructFieldInfo(
      EncodingField,
      false,
      true,
      EncodingFieldManifest,
      _root_.scala.None,
      _root_.scala.None,
      immutable$Map.empty[String, String],
      immutable$Map.empty[String, String],
      None
    ),
    new ThriftStructFieldInfo(
      DefinitionLevelsByteLengthField,
      false,
      true,
      DefinitionLevelsByteLengthFieldManifest,
      _root_.scala.None,
      _root_.scala.None,
      immutable$Map.empty[String, String],
      immutable$Map.empty[String, String],
      None
    ),
    new ThriftStructFieldInfo(
      RepetitionLevelsByteLengthField,
      false,
      true,
      RepetitionLevelsByteLengthFieldManifest,
      _root_.scala.None,
      _root_.scala.None,
      immutable$Map.empty[String, String],
      immutable$Map.empty[String, String],
      None
    ),
    new ThriftStructFieldInfo(
      IsCompressedField,
      false,
      false,
      IsCompressedFieldManifest,
      _root_.scala.None,
      _root_.scala.None,
      immutable$Map.empty[String, String],
      immutable$Map.empty[String, String],
      Some[Boolean](true)
    ),
    new ThriftStructFieldInfo(
      StatisticsField,
      true,
      false,
      StatisticsFieldManifest,
      _root_.scala.None,
      _root_.scala.None,
      immutable$Map.empty[String, String],
      immutable$Map.empty[String, String],
      None
    )
  )

  lazy val structAnnotations: immutable$Map[String, String] =
    immutable$Map.empty[String, String]

  /**
   * Checks that all required fields are non-null.
   */
  def validate(_item: DataPageHeaderV2): Unit = {
    if (_item.encoding == null) throw new TProtocolException("Required field encoding cannot be null")
  }

  def withoutPassthroughFields(original: DataPageHeaderV2): DataPageHeaderV2 =
    new Immutable(
      numValues =
        {
          val field = original.numValues
          field
        },
      numNulls =
        {
          val field = original.numNulls
          field
        },
      numRows =
        {
          val field = original.numRows
          field
        },
      encoding =
        {
          val field = original.encoding
          field
        },
      definitionLevelsByteLength =
        {
          val field = original.definitionLevelsByteLength
          field
        },
      repetitionLevelsByteLength =
        {
          val field = original.repetitionLevelsByteLength
          field
        },
      isCompressed =
        {
          val field = original.isCompressed
          field
        },
      statistics =
        {
          val field = original.statistics
          field.map { field =>
            parquet.format.Statistics.withoutPassthroughFields(field)
          }
        }
    )

  override def encode(_item: DataPageHeaderV2, _oproto: TProtocol): Unit = {
    _item.write(_oproto)
  }


  private[this] def lazyDecode(_iprot: LazyTProtocol): DataPageHeaderV2 = {

    var numValues: Int = 0
    var _got_numValues = false
    var numNulls: Int = 0
    var _got_numNulls = false
    var numRows: Int = 0
    var _got_numRows = false
    var encoding: parquet.format.Encoding = null
    var _got_encoding = false
    var definitionLevelsByteLength: Int = 0
    var _got_definitionLevelsByteLength = false
    var repetitionLevelsByteLength: Int = 0
    var _got_repetitionLevelsByteLength = false
    var isCompressed: Boolean = true
    var statistics: Option[parquet.format.Statistics] = None

    var _passthroughFields: Builder[(Short, TFieldBlob), immutable$Map[Short, TFieldBlob]] = null
    var _done = false
    val _start_offset = _iprot.offset

    _iprot.readStructBegin()
    while (!_done) {
      val _field = _iprot.readFieldBegin()
      if (_field.`type` == TType.STOP) {
        _done = true
      } else {
        _field.id match {
          case 1 =>
            _field.`type` match {
              case TType.I32 =>
    
                numValues = readNumValuesValue(_iprot)
                _got_numValues = true
              case _actualType =>
                val _expectedType = TType.I32
                throw new TProtocolException(
                  "Received wrong type for field 'numValues' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 2 =>
            _field.`type` match {
              case TType.I32 =>
    
                numNulls = readNumNullsValue(_iprot)
                _got_numNulls = true
              case _actualType =>
                val _expectedType = TType.I32
                throw new TProtocolException(
                  "Received wrong type for field 'numNulls' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 3 =>
            _field.`type` match {
              case TType.I32 =>
    
                numRows = readNumRowsValue(_iprot)
                _got_numRows = true
              case _actualType =>
                val _expectedType = TType.I32
                throw new TProtocolException(
                  "Received wrong type for field 'numRows' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 4 =>
            _field.`type` match {
              case TType.I32 | TType.ENUM =>
    
                encoding = readEncodingValue(_iprot)
                _got_encoding = true
              case _actualType =>
                val _expectedType = TType.ENUM
                throw new TProtocolException(
                  "Received wrong type for field 'encoding' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 5 =>
            _field.`type` match {
              case TType.I32 =>
    
                definitionLevelsByteLength = readDefinitionLevelsByteLengthValue(_iprot)
                _got_definitionLevelsByteLength = true
              case _actualType =>
                val _expectedType = TType.I32
                throw new TProtocolException(
                  "Received wrong type for field 'definitionLevelsByteLength' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 6 =>
            _field.`type` match {
              case TType.I32 =>
    
                repetitionLevelsByteLength = readRepetitionLevelsByteLengthValue(_iprot)
                _got_repetitionLevelsByteLength = true
              case _actualType =>
                val _expectedType = TType.I32
                throw new TProtocolException(
                  "Received wrong type for field 'repetitionLevelsByteLength' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 7 =>
            _field.`type` match {
              case TType.BOOL =>
    
                isCompressed = readIsCompressedValue(_iprot)
              case _actualType =>
                val _expectedType = TType.BOOL
                throw new TProtocolException(
                  "Received wrong type for field 'isCompressed' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 8 =>
            _field.`type` match {
              case TType.STRUCT =>
    
                statistics = Some(readStatisticsValue(_iprot))
              case _actualType =>
                val _expectedType = TType.STRUCT
                throw new TProtocolException(
                  "Received wrong type for field 'statistics' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case _ =>
            if (_passthroughFields == null)
              _passthroughFields = immutable$Map.newBuilder[Short, TFieldBlob]
            _passthroughFields += (_field.id -> TFieldBlob.read(_field, _iprot))
        }
        _iprot.readFieldEnd()
      }
    }
    _iprot.readStructEnd()

    if (!_got_numValues) throw new TProtocolException("Required field 'numValues' was not found in serialized data for struct DataPageHeaderV2")
    if (!_got_numNulls) throw new TProtocolException("Required field 'numNulls' was not found in serialized data for struct DataPageHeaderV2")
    if (!_got_numRows) throw new TProtocolException("Required field 'numRows' was not found in serialized data for struct DataPageHeaderV2")
    if (!_got_encoding) throw new TProtocolException("Required field 'encoding' was not found in serialized data for struct DataPageHeaderV2")
    if (!_got_definitionLevelsByteLength) throw new TProtocolException("Required field 'definitionLevelsByteLength' was not found in serialized data for struct DataPageHeaderV2")
    if (!_got_repetitionLevelsByteLength) throw new TProtocolException("Required field 'repetitionLevelsByteLength' was not found in serialized data for struct DataPageHeaderV2")
    new LazyImmutable(
      _iprot,
      _iprot.buffer,
      _start_offset,
      _iprot.offset,
      numValues,
      numNulls,
      numRows,
      encoding,
      definitionLevelsByteLength,
      repetitionLevelsByteLength,
      isCompressed,
      statistics,
      if (_passthroughFields == null)
        NoPassthroughFields
      else
        _passthroughFields.result()
    )
  }

  override def decode(_iprot: TProtocol): DataPageHeaderV2 =
    _iprot match {
      case i: LazyTProtocol => lazyDecode(i)
      case i => eagerDecode(i)
    }

  private[format] def eagerDecode(_iprot: TProtocol): DataPageHeaderV2 = {
    var numValues: Int = 0
    var _got_numValues = false
    var numNulls: Int = 0
    var _got_numNulls = false
    var numRows: Int = 0
    var _got_numRows = false
    var encoding: parquet.format.Encoding = null
    var _got_encoding = false
    var definitionLevelsByteLength: Int = 0
    var _got_definitionLevelsByteLength = false
    var repetitionLevelsByteLength: Int = 0
    var _got_repetitionLevelsByteLength = false
    var isCompressed: Boolean = true
    var statistics: _root_.scala.Option[parquet.format.Statistics] = _root_.scala.None
    var _passthroughFields: Builder[(Short, TFieldBlob), immutable$Map[Short, TFieldBlob]] = null
    var _done = false

    _iprot.readStructBegin()
    while (!_done) {
      val _field = _iprot.readFieldBegin()
      if (_field.`type` == TType.STOP) {
        _done = true
      } else {
        _field.id match {
          case 1 =>
            _field.`type` match {
              case TType.I32 =>
                numValues = readNumValuesValue(_iprot)
                _got_numValues = true
              case _actualType =>
                val _expectedType = TType.I32
                throw new TProtocolException(
                  "Received wrong type for field 'numValues' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 2 =>
            _field.`type` match {
              case TType.I32 =>
                numNulls = readNumNullsValue(_iprot)
                _got_numNulls = true
              case _actualType =>
                val _expectedType = TType.I32
                throw new TProtocolException(
                  "Received wrong type for field 'numNulls' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 3 =>
            _field.`type` match {
              case TType.I32 =>
                numRows = readNumRowsValue(_iprot)
                _got_numRows = true
              case _actualType =>
                val _expectedType = TType.I32
                throw new TProtocolException(
                  "Received wrong type for field 'numRows' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 4 =>
            _field.`type` match {
              case TType.I32 | TType.ENUM =>
                encoding = readEncodingValue(_iprot)
                _got_encoding = true
              case _actualType =>
                val _expectedType = TType.ENUM
                throw new TProtocolException(
                  "Received wrong type for field 'encoding' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 5 =>
            _field.`type` match {
              case TType.I32 =>
                definitionLevelsByteLength = readDefinitionLevelsByteLengthValue(_iprot)
                _got_definitionLevelsByteLength = true
              case _actualType =>
                val _expectedType = TType.I32
                throw new TProtocolException(
                  "Received wrong type for field 'definitionLevelsByteLength' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 6 =>
            _field.`type` match {
              case TType.I32 =>
                repetitionLevelsByteLength = readRepetitionLevelsByteLengthValue(_iprot)
                _got_repetitionLevelsByteLength = true
              case _actualType =>
                val _expectedType = TType.I32
                throw new TProtocolException(
                  "Received wrong type for field 'repetitionLevelsByteLength' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 7 =>
            _field.`type` match {
              case TType.BOOL =>
                isCompressed = readIsCompressedValue(_iprot)
              case _actualType =>
                val _expectedType = TType.BOOL
                throw new TProtocolException(
                  "Received wrong type for field 'isCompressed' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 8 =>
            _field.`type` match {
              case TType.STRUCT =>
                statistics = _root_.scala.Some(readStatisticsValue(_iprot))
              case _actualType =>
                val _expectedType = TType.STRUCT
                throw new TProtocolException(
                  "Received wrong type for field 'statistics' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case _ =>
            if (_passthroughFields == null)
              _passthroughFields = immutable$Map.newBuilder[Short, TFieldBlob]
            _passthroughFields += (_field.id -> TFieldBlob.read(_field, _iprot))
        }
        _iprot.readFieldEnd()
      }
    }
    _iprot.readStructEnd()

    if (!_got_numValues) throw new TProtocolException("Required field 'numValues' was not found in serialized data for struct DataPageHeaderV2")
    if (!_got_numNulls) throw new TProtocolException("Required field 'numNulls' was not found in serialized data for struct DataPageHeaderV2")
    if (!_got_numRows) throw new TProtocolException("Required field 'numRows' was not found in serialized data for struct DataPageHeaderV2")
    if (!_got_encoding) throw new TProtocolException("Required field 'encoding' was not found in serialized data for struct DataPageHeaderV2")
    if (!_got_definitionLevelsByteLength) throw new TProtocolException("Required field 'definitionLevelsByteLength' was not found in serialized data for struct DataPageHeaderV2")
    if (!_got_repetitionLevelsByteLength) throw new TProtocolException("Required field 'repetitionLevelsByteLength' was not found in serialized data for struct DataPageHeaderV2")
    new Immutable(
      numValues,
      numNulls,
      numRows,
      encoding,
      definitionLevelsByteLength,
      repetitionLevelsByteLength,
      isCompressed,
      statistics,
      if (_passthroughFields == null)
        NoPassthroughFields
      else
        _passthroughFields.result()
    )
  }

  def apply(
    numValues: Int,
    numNulls: Int,
    numRows: Int,
    encoding: parquet.format.Encoding,
    definitionLevelsByteLength: Int,
    repetitionLevelsByteLength: Int,
    isCompressed: Boolean = true,
    statistics: _root_.scala.Option[parquet.format.Statistics] = _root_.scala.None
  ): DataPageHeaderV2 =
    new Immutable(
      numValues,
      numNulls,
      numRows,
      encoding,
      definitionLevelsByteLength,
      repetitionLevelsByteLength,
      isCompressed,
      statistics
    )

  def unapply(_item: DataPageHeaderV2): _root_.scala.Option[_root_.scala.Tuple8[Int, Int, Int, parquet.format.Encoding, Int, Int, Boolean, Option[parquet.format.Statistics]]] = _root_.scala.Some(_item.toTuple)


  @inline private[format] def readNumValuesValue(_iprot: TProtocol): Int = {
    _iprot.readI32()
  }

  @inline private def writeNumValuesField(numValues_item: Int, _oprot: TProtocol): Unit = {
    _oprot.writeFieldBegin(NumValuesField)
    writeNumValuesValue(numValues_item, _oprot)
    _oprot.writeFieldEnd()
  }

  @inline private def writeNumValuesValue(numValues_item: Int, _oprot: TProtocol): Unit = {
    _oprot.writeI32(numValues_item)
  }

  @inline private[format] def readNumNullsValue(_iprot: TProtocol): Int = {
    _iprot.readI32()
  }

  @inline private def writeNumNullsField(numNulls_item: Int, _oprot: TProtocol): Unit = {
    _oprot.writeFieldBegin(NumNullsField)
    writeNumNullsValue(numNulls_item, _oprot)
    _oprot.writeFieldEnd()
  }

  @inline private def writeNumNullsValue(numNulls_item: Int, _oprot: TProtocol): Unit = {
    _oprot.writeI32(numNulls_item)
  }

  @inline private[format] def readNumRowsValue(_iprot: TProtocol): Int = {
    _iprot.readI32()
  }

  @inline private def writeNumRowsField(numRows_item: Int, _oprot: TProtocol): Unit = {
    _oprot.writeFieldBegin(NumRowsField)
    writeNumRowsValue(numRows_item, _oprot)
    _oprot.writeFieldEnd()
  }

  @inline private def writeNumRowsValue(numRows_item: Int, _oprot: TProtocol): Unit = {
    _oprot.writeI32(numRows_item)
  }

  @inline private[format] def readEncodingValue(_iprot: TProtocol): parquet.format.Encoding = {
    parquet.format.Encoding.getOrUnknown(_iprot.readI32())
  }

  @inline private def writeEncodingField(encoding_item: parquet.format.Encoding, _oprot: TProtocol): Unit = {
    _oprot.writeFieldBegin(EncodingFieldI32)
    writeEncodingValue(encoding_item, _oprot)
    _oprot.writeFieldEnd()
  }

  @inline private def writeEncodingValue(encoding_item: parquet.format.Encoding, _oprot: TProtocol): Unit = {
    _oprot.writeI32(encoding_item.value)
  }

  @inline private[format] def readDefinitionLevelsByteLengthValue(_iprot: TProtocol): Int = {
    _iprot.readI32()
  }

  @inline private def writeDefinitionLevelsByteLengthField(definitionLevelsByteLength_item: Int, _oprot: TProtocol): Unit = {
    _oprot.writeFieldBegin(DefinitionLevelsByteLengthField)
    writeDefinitionLevelsByteLengthValue(definitionLevelsByteLength_item, _oprot)
    _oprot.writeFieldEnd()
  }

  @inline private def writeDefinitionLevelsByteLengthValue(definitionLevelsByteLength_item: Int, _oprot: TProtocol): Unit = {
    _oprot.writeI32(definitionLevelsByteLength_item)
  }

  @inline private[format] def readRepetitionLevelsByteLengthValue(_iprot: TProtocol): Int = {
    _iprot.readI32()
  }

  @inline private def writeRepetitionLevelsByteLengthField(repetitionLevelsByteLength_item: Int, _oprot: TProtocol): Unit = {
    _oprot.writeFieldBegin(RepetitionLevelsByteLengthField)
    writeRepetitionLevelsByteLengthValue(repetitionLevelsByteLength_item, _oprot)
    _oprot.writeFieldEnd()
  }

  @inline private def writeRepetitionLevelsByteLengthValue(repetitionLevelsByteLength_item: Int, _oprot: TProtocol): Unit = {
    _oprot.writeI32(repetitionLevelsByteLength_item)
  }

  @inline private[format] def readIsCompressedValue(_iprot: TProtocol): Boolean = {
    _iprot.readBool()
  }

  @inline private def writeIsCompressedField(isCompressed_item: Boolean, _oprot: TProtocol): Unit = {
    _oprot.writeFieldBegin(IsCompressedField)
    writeIsCompressedValue(isCompressed_item, _oprot)
    _oprot.writeFieldEnd()
  }

  @inline private def writeIsCompressedValue(isCompressed_item: Boolean, _oprot: TProtocol): Unit = {
    _oprot.writeBool(isCompressed_item)
  }

  @inline private[format] def readStatisticsValue(_iprot: TProtocol): parquet.format.Statistics = {
    parquet.format.Statistics.decode(_iprot)
  }

  @inline private def writeStatisticsField(statistics_item: parquet.format.Statistics, _oprot: TProtocol): Unit = {
    _oprot.writeFieldBegin(StatisticsField)
    writeStatisticsValue(statistics_item, _oprot)
    _oprot.writeFieldEnd()
  }

  @inline private def writeStatisticsValue(statistics_item: parquet.format.Statistics, _oprot: TProtocol): Unit = {
    statistics_item.write(_oprot)
  }


  object Immutable extends ThriftStructCodec3[DataPageHeaderV2] {
    override def encode(_item: DataPageHeaderV2, _oproto: TProtocol): Unit = { _item.write(_oproto) }
    override def decode(_iprot: TProtocol): DataPageHeaderV2 = DataPageHeaderV2.decode(_iprot)
    override lazy val metaData: ThriftStructMetaData[DataPageHeaderV2] = DataPageHeaderV2.metaData
  }

  /**
   * The default read-only implementation of DataPageHeaderV2.  You typically should not need to
   * directly reference this class; instead, use the DataPageHeaderV2.apply method to construct
   * new instances.
   */
  class Immutable(
      val numValues: Int,
      val numNulls: Int,
      val numRows: Int,
      val encoding: parquet.format.Encoding,
      val definitionLevelsByteLength: Int,
      val repetitionLevelsByteLength: Int,
      val isCompressed: Boolean,
      val statistics: _root_.scala.Option[parquet.format.Statistics],
      override val _passthroughFields: immutable$Map[Short, TFieldBlob])
    extends DataPageHeaderV2 {
    def this(
      numValues: Int,
      numNulls: Int,
      numRows: Int,
      encoding: parquet.format.Encoding,
      definitionLevelsByteLength: Int,
      repetitionLevelsByteLength: Int,
      isCompressed: Boolean = true,
      statistics: _root_.scala.Option[parquet.format.Statistics] = _root_.scala.None
    ) = this(
      numValues,
      numNulls,
      numRows,
      encoding,
      definitionLevelsByteLength,
      repetitionLevelsByteLength,
      isCompressed,
      statistics,
      Map.empty
    )
  }

  /**
   * This is another Immutable, this however keeps strings as lazy values that are lazily decoded from the backing
   * array byte on read.
   */
  private[this] class LazyImmutable(
      _proto: LazyTProtocol,
      _buf: Array[Byte],
      _start_offset: Int,
      _end_offset: Int,
      val numValues: Int,
      val numNulls: Int,
      val numRows: Int,
      val encoding: parquet.format.Encoding,
      val definitionLevelsByteLength: Int,
      val repetitionLevelsByteLength: Int,
      val isCompressed: Boolean,
      val statistics: _root_.scala.Option[parquet.format.Statistics],
      override val _passthroughFields: immutable$Map[Short, TFieldBlob])
    extends DataPageHeaderV2 {

    override def write(_oprot: TProtocol): Unit = {
      _oprot match {
        case i: LazyTProtocol => i.writeRaw(_buf, _start_offset, _end_offset - _start_offset)
        case _ => super.write(_oprot)
      }
    }


    /**
     * Override the super hash code to make it a lazy val rather than def.
     *
     * Calculating the hash code can be expensive, caching it where possible
     * can provide significant performance wins. (Key in a hash map for instance)
     * Usually not safe since the normal constructor will accept a mutable map or
     * set as an arg
     * Here however we control how the class is generated from serialized data.
     * With the class private and the contract that we throw away our mutable references
     * having the hash code lazy here is safe.
     */
    override lazy val hashCode = super.hashCode
  }

  /**
   * This Proxy trait allows you to extend the DataPageHeaderV2 trait with additional state or
   * behavior and implement the read-only methods from DataPageHeaderV2 using an underlying
   * instance.
   */
  trait Proxy extends DataPageHeaderV2 {
    protected def _underlying_DataPageHeaderV2: DataPageHeaderV2
    override def numValues: Int = _underlying_DataPageHeaderV2.numValues
    override def numNulls: Int = _underlying_DataPageHeaderV2.numNulls
    override def numRows: Int = _underlying_DataPageHeaderV2.numRows
    override def encoding: parquet.format.Encoding = _underlying_DataPageHeaderV2.encoding
    override def definitionLevelsByteLength: Int = _underlying_DataPageHeaderV2.definitionLevelsByteLength
    override def repetitionLevelsByteLength: Int = _underlying_DataPageHeaderV2.repetitionLevelsByteLength
    override def isCompressed: Boolean = _underlying_DataPageHeaderV2.isCompressed
    override def statistics: _root_.scala.Option[parquet.format.Statistics] = _underlying_DataPageHeaderV2.statistics
    override def _passthroughFields = _underlying_DataPageHeaderV2._passthroughFields
  }
}

/**
 * Prefer the companion object's [[parquet.format.DataPageHeaderV2.apply]]
 * for construction if you don't need to specify passthrough fields.
 */
trait DataPageHeaderV2
  extends ThriftStruct
  with _root_.scala.Product8[Int, Int, Int, parquet.format.Encoding, Int, Int, Boolean, Option[parquet.format.Statistics]]
  with HasThriftStructCodec3[DataPageHeaderV2]
  with java.io.Serializable
{
  import DataPageHeaderV2._

  def numValues: Int
  def numNulls: Int
  def numRows: Int
  def encoding: parquet.format.Encoding
  def definitionLevelsByteLength: Int
  def repetitionLevelsByteLength: Int
  def isCompressed: Boolean
  def statistics: _root_.scala.Option[parquet.format.Statistics]

  def _passthroughFields: immutable$Map[Short, TFieldBlob] = immutable$Map.empty

  def _1 = numValues
  def _2 = numNulls
  def _3 = numRows
  def _4 = encoding
  def _5 = definitionLevelsByteLength
  def _6 = repetitionLevelsByteLength
  def _7 = isCompressed
  def _8 = statistics

  def toTuple: _root_.scala.Tuple8[Int, Int, Int, parquet.format.Encoding, Int, Int, Boolean, Option[parquet.format.Statistics]] = {
    (
      numValues,
      numNulls,
      numRows,
      encoding,
      definitionLevelsByteLength,
      repetitionLevelsByteLength,
      isCompressed,
      statistics
    )
  }


  /**
   * Gets a field value encoded as a binary blob using TCompactProtocol.  If the specified field
   * is present in the passthrough map, that value is returned.  Otherwise, if the specified field
   * is known and not optional and set to None, then the field is serialized and returned.
   */
  def getFieldBlob(_fieldId: Short): _root_.scala.Option[TFieldBlob] = {
    lazy val _buff = new TMemoryBuffer(32)
    lazy val _oprot = new TCompactProtocol(_buff)
    _passthroughFields.get(_fieldId) match {
      case blob: _root_.scala.Some[TFieldBlob] => blob
      case _root_.scala.None => {
        val _fieldOpt: _root_.scala.Option[TField] =
          _fieldId match {
            case 1 =>
              if (true) {
                writeNumValuesValue(numValues, _oprot)
                _root_.scala.Some(DataPageHeaderV2.NumValuesField)
              } else {
                _root_.scala.None
              }
            case 2 =>
              if (true) {
                writeNumNullsValue(numNulls, _oprot)
                _root_.scala.Some(DataPageHeaderV2.NumNullsField)
              } else {
                _root_.scala.None
              }
            case 3 =>
              if (true) {
                writeNumRowsValue(numRows, _oprot)
                _root_.scala.Some(DataPageHeaderV2.NumRowsField)
              } else {
                _root_.scala.None
              }
            case 4 =>
              if (encoding ne null) {
                writeEncodingValue(encoding, _oprot)
                _root_.scala.Some(DataPageHeaderV2.EncodingField)
              } else {
                _root_.scala.None
              }
            case 5 =>
              if (true) {
                writeDefinitionLevelsByteLengthValue(definitionLevelsByteLength, _oprot)
                _root_.scala.Some(DataPageHeaderV2.DefinitionLevelsByteLengthField)
              } else {
                _root_.scala.None
              }
            case 6 =>
              if (true) {
                writeRepetitionLevelsByteLengthValue(repetitionLevelsByteLength, _oprot)
                _root_.scala.Some(DataPageHeaderV2.RepetitionLevelsByteLengthField)
              } else {
                _root_.scala.None
              }
            case 7 =>
              if (true) {
                writeIsCompressedValue(isCompressed, _oprot)
                _root_.scala.Some(DataPageHeaderV2.IsCompressedField)
              } else {
                _root_.scala.None
              }
            case 8 =>
              if (statistics.isDefined) {
                writeStatisticsValue(statistics.get, _oprot)
                _root_.scala.Some(DataPageHeaderV2.StatisticsField)
              } else {
                _root_.scala.None
              }
            case _ => _root_.scala.None
          }
        _fieldOpt match {
          case _root_.scala.Some(_field) =>
            _root_.scala.Some(TFieldBlob(_field, Buf.ByteArray.Owned(_buff.getArray())))
          case _root_.scala.None =>
            _root_.scala.None
        }
      }
    }
  }

  /**
   * Collects TCompactProtocol-encoded field values according to `getFieldBlob` into a map.
   */
  def getFieldBlobs(ids: TraversableOnce[Short]): immutable$Map[Short, TFieldBlob] =
    (ids flatMap { id => getFieldBlob(id) map { id -> _ } }).toMap

  /**
   * Sets a field using a TCompactProtocol-encoded binary blob.  If the field is a known
   * field, the blob is decoded and the field is set to the decoded value.  If the field
   * is unknown and passthrough fields are enabled, then the blob will be stored in
   * _passthroughFields.
   */
  def setField(_blob: TFieldBlob): DataPageHeaderV2 = {
    var numValues: Int = this.numValues
    var numNulls: Int = this.numNulls
    var numRows: Int = this.numRows
    var encoding: parquet.format.Encoding = this.encoding
    var definitionLevelsByteLength: Int = this.definitionLevelsByteLength
    var repetitionLevelsByteLength: Int = this.repetitionLevelsByteLength
    var isCompressed: Boolean = this.isCompressed
    var statistics: _root_.scala.Option[parquet.format.Statistics] = this.statistics
    var _passthroughFields = this._passthroughFields
    _blob.id match {
      case 1 =>
        numValues = readNumValuesValue(_blob.read)
      case 2 =>
        numNulls = readNumNullsValue(_blob.read)
      case 3 =>
        numRows = readNumRowsValue(_blob.read)
      case 4 =>
        encoding = readEncodingValue(_blob.read)
      case 5 =>
        definitionLevelsByteLength = readDefinitionLevelsByteLengthValue(_blob.read)
      case 6 =>
        repetitionLevelsByteLength = readRepetitionLevelsByteLengthValue(_blob.read)
      case 7 =>
        isCompressed = readIsCompressedValue(_blob.read)
      case 8 =>
        statistics = _root_.scala.Some(readStatisticsValue(_blob.read))
      case _ => _passthroughFields += (_blob.id -> _blob)
    }
    new Immutable(
      numValues,
      numNulls,
      numRows,
      encoding,
      definitionLevelsByteLength,
      repetitionLevelsByteLength,
      isCompressed,
      statistics,
      _passthroughFields
    )
  }

  /**
   * If the specified field is optional, it is set to None.  Otherwise, if the field is
   * known, it is reverted to its default value; if the field is unknown, it is removed
   * from the passthroughFields map, if present.
   */
  def unsetField(_fieldId: Short): DataPageHeaderV2 = {
    var numValues: Int = this.numValues
    var numNulls: Int = this.numNulls
    var numRows: Int = this.numRows
    var encoding: parquet.format.Encoding = this.encoding
    var definitionLevelsByteLength: Int = this.definitionLevelsByteLength
    var repetitionLevelsByteLength: Int = this.repetitionLevelsByteLength
    var isCompressed: Boolean = this.isCompressed
    var statistics: _root_.scala.Option[parquet.format.Statistics] = this.statistics

    _fieldId match {
      case 1 =>
        numValues = 0
      case 2 =>
        numNulls = 0
      case 3 =>
        numRows = 0
      case 4 =>
        encoding = null
      case 5 =>
        definitionLevelsByteLength = 0
      case 6 =>
        repetitionLevelsByteLength = 0
      case 7 =>
        isCompressed = true
      case 8 =>
        statistics = _root_.scala.None
      case _ =>
    }
    new Immutable(
      numValues,
      numNulls,
      numRows,
      encoding,
      definitionLevelsByteLength,
      repetitionLevelsByteLength,
      isCompressed,
      statistics,
      _passthroughFields - _fieldId
    )
  }

  /**
   * If the specified field is optional, it is set to None.  Otherwise, if the field is
   * known, it is reverted to its default value; if the field is unknown, it is removed
   * from the passthroughFields map, if present.
   */
  def unsetNumValues: DataPageHeaderV2 = unsetField(1)

  def unsetNumNulls: DataPageHeaderV2 = unsetField(2)

  def unsetNumRows: DataPageHeaderV2 = unsetField(3)

  def unsetEncoding: DataPageHeaderV2 = unsetField(4)

  def unsetDefinitionLevelsByteLength: DataPageHeaderV2 = unsetField(5)

  def unsetRepetitionLevelsByteLength: DataPageHeaderV2 = unsetField(6)

  def unsetIsCompressed: DataPageHeaderV2 = unsetField(7)

  def unsetStatistics: DataPageHeaderV2 = unsetField(8)


  override def write(_oprot: TProtocol): Unit = {
    DataPageHeaderV2.validate(this)
    _oprot.writeStructBegin(Struct)
    writeNumValuesField(numValues, _oprot)
    writeNumNullsField(numNulls, _oprot)
    writeNumRowsField(numRows, _oprot)
    if (encoding ne null) writeEncodingField(encoding, _oprot)
    writeDefinitionLevelsByteLengthField(definitionLevelsByteLength, _oprot)
    writeRepetitionLevelsByteLengthField(repetitionLevelsByteLength, _oprot)
    writeIsCompressedField(isCompressed, _oprot)
    if (statistics.isDefined) writeStatisticsField(statistics.get, _oprot)
    if (_passthroughFields.nonEmpty) {
      _passthroughFields.values.foreach { _.write(_oprot) }
    }
    _oprot.writeFieldStop()
    _oprot.writeStructEnd()
  }

  def copy(
    numValues: Int = this.numValues,
    numNulls: Int = this.numNulls,
    numRows: Int = this.numRows,
    encoding: parquet.format.Encoding = this.encoding,
    definitionLevelsByteLength: Int = this.definitionLevelsByteLength,
    repetitionLevelsByteLength: Int = this.repetitionLevelsByteLength,
    isCompressed: Boolean = this.isCompressed,
    statistics: _root_.scala.Option[parquet.format.Statistics] = this.statistics,
    _passthroughFields: immutable$Map[Short, TFieldBlob] = this._passthroughFields
  ): DataPageHeaderV2 =
    new Immutable(
      numValues,
      numNulls,
      numRows,
      encoding,
      definitionLevelsByteLength,
      repetitionLevelsByteLength,
      isCompressed,
      statistics,
      _passthroughFields
    )

  override def canEqual(other: Any): Boolean = other.isInstanceOf[DataPageHeaderV2]

  private def _equals(x: DataPageHeaderV2, y: DataPageHeaderV2): Boolean =
      x.productArity == y.productArity &&
      x.productIterator.sameElements(y.productIterator)

  override def equals(other: Any): Boolean =
    canEqual(other) &&
      _equals(this, other.asInstanceOf[DataPageHeaderV2]) &&
      _passthroughFields == other.asInstanceOf[DataPageHeaderV2]._passthroughFields

  override def hashCode: Int = _root_.scala.runtime.ScalaRunTime._hashCode(this)

  override def toString: String = _root_.scala.runtime.ScalaRunTime._toString(this)


  override def productArity: Int = 8

  override def productElement(n: Int): Any = n match {
    case 0 => this.numValues
    case 1 => this.numNulls
    case 2 => this.numRows
    case 3 => this.encoding
    case 4 => this.definitionLevelsByteLength
    case 5 => this.repetitionLevelsByteLength
    case 6 => this.isCompressed
    case 7 => this.statistics
    case _ => throw new IndexOutOfBoundsException(n.toString)
  }

  override def productPrefix: String = "DataPageHeaderV2"

  def _codec: ThriftStructCodec3[DataPageHeaderV2] = DataPageHeaderV2
}

