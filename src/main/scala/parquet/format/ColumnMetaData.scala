/**
 * Generated by Scrooge
 *   version: 4.16.0-SNAPSHOT
 *   rev: eb110c820bcd2734b26023f24d636bf6d37511b3
 *   built at: 20170607-185808
 */
package parquet.format

import com.twitter.io.Buf
import com.twitter.scrooge.{
  HasThriftStructCodec3,
  LazyTProtocol,
  TFieldBlob,
  ThriftException,
  ThriftStruct,
  ThriftStructCodec3,
  ThriftStructFieldInfo,
  ThriftStructMetaData,
  ThriftUtil
}
import org.apache.thrift.protocol._
import org.apache.thrift.transport.{TMemoryBuffer, TTransport, TIOStreamTransport}
import java.io.ByteArrayInputStream
import java.nio.ByteBuffer
import java.util.Arrays
import java.util.concurrent.atomic.AtomicInteger
import scala.collection.immutable.{Map => immutable$Map}
import scala.collection.mutable.Builder
import scala.collection.mutable.{
  ArrayBuffer => mutable$ArrayBuffer, Buffer => mutable$Buffer,
  HashMap => mutable$HashMap, HashSet => mutable$HashSet}
import scala.collection.{Map, Set}

/**
 * Description for column metadata
 */
object ColumnMetaData extends ThriftStructCodec3[ColumnMetaData] {
  val NoPassthroughFields: immutable$Map[Short, TFieldBlob] = immutable$Map.empty[Short, TFieldBlob]
  val Struct = new TStruct("ColumnMetaData")
  val TypeField = new TField("type", TType.ENUM, 1)
  val TypeFieldI32 = new TField("type", TType.I32, 1)
  val TypeFieldManifest = implicitly[Manifest[parquet.format.Type]]
  val EncodingsField = new TField("encodings", TType.LIST, 2)
  val EncodingsFieldManifest = implicitly[Manifest[Seq[parquet.format.Encoding]]]
  val PathInSchemaField = new TField("path_in_schema", TType.LIST, 3)
  val PathInSchemaFieldManifest = implicitly[Manifest[Seq[String]]]
  val CodecField = new TField("codec", TType.ENUM, 4)
  val CodecFieldI32 = new TField("codec", TType.I32, 4)
  val CodecFieldManifest = implicitly[Manifest[parquet.format.CompressionCodec]]
  val NumValuesField = new TField("num_values", TType.I64, 5)
  val NumValuesFieldManifest = implicitly[Manifest[Long]]
  val TotalUncompressedSizeField = new TField("total_uncompressed_size", TType.I64, 6)
  val TotalUncompressedSizeFieldManifest = implicitly[Manifest[Long]]
  val TotalCompressedSizeField = new TField("total_compressed_size", TType.I64, 7)
  val TotalCompressedSizeFieldManifest = implicitly[Manifest[Long]]
  val KeyValueMetadataField = new TField("key_value_metadata", TType.LIST, 8)
  val KeyValueMetadataFieldManifest = implicitly[Manifest[Seq[parquet.format.KeyValue]]]
  val DataPageOffsetField = new TField("data_page_offset", TType.I64, 9)
  val DataPageOffsetFieldManifest = implicitly[Manifest[Long]]
  val IndexPageOffsetField = new TField("index_page_offset", TType.I64, 10)
  val IndexPageOffsetFieldManifest = implicitly[Manifest[Long]]
  val DictionaryPageOffsetField = new TField("dictionary_page_offset", TType.I64, 11)
  val DictionaryPageOffsetFieldManifest = implicitly[Manifest[Long]]
  val StatisticsField = new TField("statistics", TType.STRUCT, 12)
  val StatisticsFieldManifest = implicitly[Manifest[parquet.format.Statistics]]
  val EncodingStatsField = new TField("encoding_stats", TType.LIST, 13)
  val EncodingStatsFieldManifest = implicitly[Manifest[Seq[parquet.format.PageEncodingStats]]]

  /**
   * Field information in declaration order.
   */
  lazy val fieldInfos: scala.List[ThriftStructFieldInfo] = scala.List[ThriftStructFieldInfo](
    new ThriftStructFieldInfo(
      TypeField,
      false,
      true,
      TypeFieldManifest,
      _root_.scala.None,
      _root_.scala.None,
      immutable$Map.empty[String, String],
      immutable$Map.empty[String, String],
      None
    ),
    new ThriftStructFieldInfo(
      EncodingsField,
      false,
      true,
      EncodingsFieldManifest,
      _root_.scala.None,
      _root_.scala.Some(implicitly[Manifest[parquet.format.Encoding]]),
      immutable$Map.empty[String, String],
      immutable$Map.empty[String, String],
      None
    ),
    new ThriftStructFieldInfo(
      PathInSchemaField,
      false,
      true,
      PathInSchemaFieldManifest,
      _root_.scala.None,
      _root_.scala.Some(implicitly[Manifest[String]]),
      immutable$Map.empty[String, String],
      immutable$Map.empty[String, String],
      None
    ),
    new ThriftStructFieldInfo(
      CodecField,
      false,
      true,
      CodecFieldManifest,
      _root_.scala.None,
      _root_.scala.None,
      immutable$Map.empty[String, String],
      immutable$Map.empty[String, String],
      None
    ),
    new ThriftStructFieldInfo(
      NumValuesField,
      false,
      true,
      NumValuesFieldManifest,
      _root_.scala.None,
      _root_.scala.None,
      immutable$Map.empty[String, String],
      immutable$Map.empty[String, String],
      None
    ),
    new ThriftStructFieldInfo(
      TotalUncompressedSizeField,
      false,
      true,
      TotalUncompressedSizeFieldManifest,
      _root_.scala.None,
      _root_.scala.None,
      immutable$Map.empty[String, String],
      immutable$Map.empty[String, String],
      None
    ),
    new ThriftStructFieldInfo(
      TotalCompressedSizeField,
      false,
      true,
      TotalCompressedSizeFieldManifest,
      _root_.scala.None,
      _root_.scala.None,
      immutable$Map.empty[String, String],
      immutable$Map.empty[String, String],
      None
    ),
    new ThriftStructFieldInfo(
      KeyValueMetadataField,
      true,
      false,
      KeyValueMetadataFieldManifest,
      _root_.scala.None,
      _root_.scala.Some(implicitly[Manifest[parquet.format.KeyValue]]),
      immutable$Map.empty[String, String],
      immutable$Map.empty[String, String],
      None
    ),
    new ThriftStructFieldInfo(
      DataPageOffsetField,
      false,
      true,
      DataPageOffsetFieldManifest,
      _root_.scala.None,
      _root_.scala.None,
      immutable$Map.empty[String, String],
      immutable$Map.empty[String, String],
      None
    ),
    new ThriftStructFieldInfo(
      IndexPageOffsetField,
      true,
      false,
      IndexPageOffsetFieldManifest,
      _root_.scala.None,
      _root_.scala.None,
      immutable$Map.empty[String, String],
      immutable$Map.empty[String, String],
      None
    ),
    new ThriftStructFieldInfo(
      DictionaryPageOffsetField,
      true,
      false,
      DictionaryPageOffsetFieldManifest,
      _root_.scala.None,
      _root_.scala.None,
      immutable$Map.empty[String, String],
      immutable$Map.empty[String, String],
      None
    ),
    new ThriftStructFieldInfo(
      StatisticsField,
      true,
      false,
      StatisticsFieldManifest,
      _root_.scala.None,
      _root_.scala.None,
      immutable$Map.empty[String, String],
      immutable$Map.empty[String, String],
      None
    ),
    new ThriftStructFieldInfo(
      EncodingStatsField,
      true,
      false,
      EncodingStatsFieldManifest,
      _root_.scala.None,
      _root_.scala.Some(implicitly[Manifest[parquet.format.PageEncodingStats]]),
      immutable$Map.empty[String, String],
      immutable$Map.empty[String, String],
      None
    )
  )

  lazy val structAnnotations: immutable$Map[String, String] =
    immutable$Map.empty[String, String]

  /**
   * Checks that all required fields are non-null.
   */
  def validate(_item: ColumnMetaData): Unit = {
    if (_item.`type` == null) throw new TProtocolException("Required field `type` cannot be null")
    if (_item.encodings == null) throw new TProtocolException("Required field encodings cannot be null")
    if (_item.pathInSchema == null) throw new TProtocolException("Required field pathInSchema cannot be null")
    if (_item.codec == null) throw new TProtocolException("Required field codec cannot be null")
  }

  def withoutPassthroughFields(original: ColumnMetaData): ColumnMetaData =
    new Immutable(
      `type` =
        {
          val field = original.`type`
          field
        },
      encodings =
        {
          val field = original.encodings
          field.map { field =>
            field
          }
        },
      pathInSchema =
        {
          val field = original.pathInSchema
          field.map { field =>
            field
          }
        },
      codec =
        {
          val field = original.codec
          field
        },
      numValues =
        {
          val field = original.numValues
          field
        },
      totalUncompressedSize =
        {
          val field = original.totalUncompressedSize
          field
        },
      totalCompressedSize =
        {
          val field = original.totalCompressedSize
          field
        },
      keyValueMetadata =
        {
          val field = original.keyValueMetadata
          field.map { field =>
            field.map { field =>
              parquet.format.KeyValue.withoutPassthroughFields(field)
            }
          }
        },
      dataPageOffset =
        {
          val field = original.dataPageOffset
          field
        },
      indexPageOffset =
        {
          val field = original.indexPageOffset
          field.map { field =>
            field
          }
        },
      dictionaryPageOffset =
        {
          val field = original.dictionaryPageOffset
          field.map { field =>
            field
          }
        },
      statistics =
        {
          val field = original.statistics
          field.map { field =>
            parquet.format.Statistics.withoutPassthroughFields(field)
          }
        },
      encodingStats =
        {
          val field = original.encodingStats
          field.map { field =>
            field.map { field =>
              parquet.format.PageEncodingStats.withoutPassthroughFields(field)
            }
          }
        }
    )

  override def encode(_item: ColumnMetaData, _oproto: TProtocol): Unit = {
    _item.write(_oproto)
  }


  private[this] def lazyDecode(_iprot: LazyTProtocol): ColumnMetaData = {

    var `type`: parquet.format.Type = null
    var _got_type = false
    var encodings: Seq[parquet.format.Encoding] = Seq[parquet.format.Encoding]()
    var _got_encodings = false
    var pathInSchema: Seq[String] = Seq[String]()
    var _got_pathInSchema = false
    var codec: parquet.format.CompressionCodec = null
    var _got_codec = false
    var numValues: Long = 0L
    var _got_numValues = false
    var totalUncompressedSize: Long = 0L
    var _got_totalUncompressedSize = false
    var totalCompressedSize: Long = 0L
    var _got_totalCompressedSize = false
    var keyValueMetadata: Option[Seq[parquet.format.KeyValue]] = None
    var dataPageOffset: Long = 0L
    var _got_dataPageOffset = false
    var index_page_offsetOffset: Int = -1
    var dictionary_page_offsetOffset: Int = -1
    var statistics: Option[parquet.format.Statistics] = None
    var encodingStats: Option[Seq[parquet.format.PageEncodingStats]] = None

    var _passthroughFields: Builder[(Short, TFieldBlob), immutable$Map[Short, TFieldBlob]] = null
    var _done = false
    val _start_offset = _iprot.offset

    _iprot.readStructBegin()
    while (!_done) {
      val _field = _iprot.readFieldBegin()
      if (_field.`type` == TType.STOP) {
        _done = true
      } else {
        _field.id match {
          case 1 =>
            _field.`type` match {
              case TType.I32 | TType.ENUM =>
    
                `type` = readTypeValue(_iprot)
                _got_type = true
              case _actualType =>
                val _expectedType = TType.ENUM
                throw new TProtocolException(
                  "Received wrong type for field '`type`' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 2 =>
            _field.`type` match {
              case TType.LIST =>
    
                encodings = readEncodingsValue(_iprot)
                _got_encodings = true
              case _actualType =>
                val _expectedType = TType.LIST
                throw new TProtocolException(
                  "Received wrong type for field 'encodings' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 3 =>
            _field.`type` match {
              case TType.LIST =>
    
                pathInSchema = readPathInSchemaValue(_iprot)
                _got_pathInSchema = true
              case _actualType =>
                val _expectedType = TType.LIST
                throw new TProtocolException(
                  "Received wrong type for field 'pathInSchema' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 4 =>
            _field.`type` match {
              case TType.I32 | TType.ENUM =>
    
                codec = readCodecValue(_iprot)
                _got_codec = true
              case _actualType =>
                val _expectedType = TType.ENUM
                throw new TProtocolException(
                  "Received wrong type for field 'codec' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 5 =>
            _field.`type` match {
              case TType.I64 =>
    
                numValues = readNumValuesValue(_iprot)
                _got_numValues = true
              case _actualType =>
                val _expectedType = TType.I64
                throw new TProtocolException(
                  "Received wrong type for field 'numValues' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 6 =>
            _field.`type` match {
              case TType.I64 =>
    
                totalUncompressedSize = readTotalUncompressedSizeValue(_iprot)
                _got_totalUncompressedSize = true
              case _actualType =>
                val _expectedType = TType.I64
                throw new TProtocolException(
                  "Received wrong type for field 'totalUncompressedSize' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 7 =>
            _field.`type` match {
              case TType.I64 =>
    
                totalCompressedSize = readTotalCompressedSizeValue(_iprot)
                _got_totalCompressedSize = true
              case _actualType =>
                val _expectedType = TType.I64
                throw new TProtocolException(
                  "Received wrong type for field 'totalCompressedSize' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 8 =>
            _field.`type` match {
              case TType.LIST =>
    
                keyValueMetadata = Some(readKeyValueMetadataValue(_iprot))
              case _actualType =>
                val _expectedType = TType.LIST
                throw new TProtocolException(
                  "Received wrong type for field 'keyValueMetadata' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 9 =>
            _field.`type` match {
              case TType.I64 =>
    
                dataPageOffset = readDataPageOffsetValue(_iprot)
                _got_dataPageOffset = true
              case _actualType =>
                val _expectedType = TType.I64
                throw new TProtocolException(
                  "Received wrong type for field 'dataPageOffset' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 10 =>
            _field.`type` match {
              case TType.I64 =>
                index_page_offsetOffset = _iprot.offsetSkipI64
    
              case _actualType =>
                val _expectedType = TType.I64
                throw new TProtocolException(
                  "Received wrong type for field 'indexPageOffset' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 11 =>
            _field.`type` match {
              case TType.I64 =>
                dictionary_page_offsetOffset = _iprot.offsetSkipI64
    
              case _actualType =>
                val _expectedType = TType.I64
                throw new TProtocolException(
                  "Received wrong type for field 'dictionaryPageOffset' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 12 =>
            _field.`type` match {
              case TType.STRUCT =>
    
                statistics = Some(readStatisticsValue(_iprot))
              case _actualType =>
                val _expectedType = TType.STRUCT
                throw new TProtocolException(
                  "Received wrong type for field 'statistics' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 13 =>
            _field.`type` match {
              case TType.LIST =>
    
                encodingStats = Some(readEncodingStatsValue(_iprot))
              case _actualType =>
                val _expectedType = TType.LIST
                throw new TProtocolException(
                  "Received wrong type for field 'encodingStats' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case _ =>
            if (_passthroughFields == null)
              _passthroughFields = immutable$Map.newBuilder[Short, TFieldBlob]
            _passthroughFields += (_field.id -> TFieldBlob.read(_field, _iprot))
        }
        _iprot.readFieldEnd()
      }
    }
    _iprot.readStructEnd()

    if (!_got_type) throw new TProtocolException("Required field '`type`' was not found in serialized data for struct ColumnMetaData")
    if (!_got_encodings) throw new TProtocolException("Required field 'encodings' was not found in serialized data for struct ColumnMetaData")
    if (!_got_pathInSchema) throw new TProtocolException("Required field 'pathInSchema' was not found in serialized data for struct ColumnMetaData")
    if (!_got_codec) throw new TProtocolException("Required field 'codec' was not found in serialized data for struct ColumnMetaData")
    if (!_got_numValues) throw new TProtocolException("Required field 'numValues' was not found in serialized data for struct ColumnMetaData")
    if (!_got_totalUncompressedSize) throw new TProtocolException("Required field 'totalUncompressedSize' was not found in serialized data for struct ColumnMetaData")
    if (!_got_totalCompressedSize) throw new TProtocolException("Required field 'totalCompressedSize' was not found in serialized data for struct ColumnMetaData")
    if (!_got_dataPageOffset) throw new TProtocolException("Required field 'dataPageOffset' was not found in serialized data for struct ColumnMetaData")
    new LazyImmutable(
      _iprot,
      _iprot.buffer,
      _start_offset,
      _iprot.offset,
      `type`,
      encodings,
      pathInSchema,
      codec,
      numValues,
      totalUncompressedSize,
      totalCompressedSize,
      keyValueMetadata,
      dataPageOffset,
      index_page_offsetOffset,
      dictionary_page_offsetOffset,
      statistics,
      encodingStats,
      if (_passthroughFields == null)
        NoPassthroughFields
      else
        _passthroughFields.result()
    )
  }

  override def decode(_iprot: TProtocol): ColumnMetaData =
    _iprot match {
      case i: LazyTProtocol => lazyDecode(i)
      case i => eagerDecode(i)
    }

  private[format] def eagerDecode(_iprot: TProtocol): ColumnMetaData = {
    var `type`: parquet.format.Type = null
    var _got_type = false
    var encodings: Seq[parquet.format.Encoding] = Seq[parquet.format.Encoding]()
    var _got_encodings = false
    var pathInSchema: Seq[String] = Seq[String]()
    var _got_pathInSchema = false
    var codec: parquet.format.CompressionCodec = null
    var _got_codec = false
    var numValues: Long = 0L
    var _got_numValues = false
    var totalUncompressedSize: Long = 0L
    var _got_totalUncompressedSize = false
    var totalCompressedSize: Long = 0L
    var _got_totalCompressedSize = false
    var keyValueMetadata: _root_.scala.Option[Seq[parquet.format.KeyValue]] = _root_.scala.None
    var dataPageOffset: Long = 0L
    var _got_dataPageOffset = false
    var indexPageOffset: _root_.scala.Option[Long] = _root_.scala.None
    var dictionaryPageOffset: _root_.scala.Option[Long] = _root_.scala.None
    var statistics: _root_.scala.Option[parquet.format.Statistics] = _root_.scala.None
    var encodingStats: _root_.scala.Option[Seq[parquet.format.PageEncodingStats]] = _root_.scala.None
    var _passthroughFields: Builder[(Short, TFieldBlob), immutable$Map[Short, TFieldBlob]] = null
    var _done = false

    _iprot.readStructBegin()
    while (!_done) {
      val _field = _iprot.readFieldBegin()
      if (_field.`type` == TType.STOP) {
        _done = true
      } else {
        _field.id match {
          case 1 =>
            _field.`type` match {
              case TType.I32 | TType.ENUM =>
                `type` = readTypeValue(_iprot)
                _got_type = true
              case _actualType =>
                val _expectedType = TType.ENUM
                throw new TProtocolException(
                  "Received wrong type for field '`type`' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 2 =>
            _field.`type` match {
              case TType.LIST =>
                encodings = readEncodingsValue(_iprot)
                _got_encodings = true
              case _actualType =>
                val _expectedType = TType.LIST
                throw new TProtocolException(
                  "Received wrong type for field 'encodings' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 3 =>
            _field.`type` match {
              case TType.LIST =>
                pathInSchema = readPathInSchemaValue(_iprot)
                _got_pathInSchema = true
              case _actualType =>
                val _expectedType = TType.LIST
                throw new TProtocolException(
                  "Received wrong type for field 'pathInSchema' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 4 =>
            _field.`type` match {
              case TType.I32 | TType.ENUM =>
                codec = readCodecValue(_iprot)
                _got_codec = true
              case _actualType =>
                val _expectedType = TType.ENUM
                throw new TProtocolException(
                  "Received wrong type for field 'codec' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 5 =>
            _field.`type` match {
              case TType.I64 =>
                numValues = readNumValuesValue(_iprot)
                _got_numValues = true
              case _actualType =>
                val _expectedType = TType.I64
                throw new TProtocolException(
                  "Received wrong type for field 'numValues' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 6 =>
            _field.`type` match {
              case TType.I64 =>
                totalUncompressedSize = readTotalUncompressedSizeValue(_iprot)
                _got_totalUncompressedSize = true
              case _actualType =>
                val _expectedType = TType.I64
                throw new TProtocolException(
                  "Received wrong type for field 'totalUncompressedSize' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 7 =>
            _field.`type` match {
              case TType.I64 =>
                totalCompressedSize = readTotalCompressedSizeValue(_iprot)
                _got_totalCompressedSize = true
              case _actualType =>
                val _expectedType = TType.I64
                throw new TProtocolException(
                  "Received wrong type for field 'totalCompressedSize' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 8 =>
            _field.`type` match {
              case TType.LIST =>
                keyValueMetadata = _root_.scala.Some(readKeyValueMetadataValue(_iprot))
              case _actualType =>
                val _expectedType = TType.LIST
                throw new TProtocolException(
                  "Received wrong type for field 'keyValueMetadata' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 9 =>
            _field.`type` match {
              case TType.I64 =>
                dataPageOffset = readDataPageOffsetValue(_iprot)
                _got_dataPageOffset = true
              case _actualType =>
                val _expectedType = TType.I64
                throw new TProtocolException(
                  "Received wrong type for field 'dataPageOffset' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 10 =>
            _field.`type` match {
              case TType.I64 =>
                indexPageOffset = _root_.scala.Some(readIndexPageOffsetValue(_iprot))
              case _actualType =>
                val _expectedType = TType.I64
                throw new TProtocolException(
                  "Received wrong type for field 'indexPageOffset' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 11 =>
            _field.`type` match {
              case TType.I64 =>
                dictionaryPageOffset = _root_.scala.Some(readDictionaryPageOffsetValue(_iprot))
              case _actualType =>
                val _expectedType = TType.I64
                throw new TProtocolException(
                  "Received wrong type for field 'dictionaryPageOffset' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 12 =>
            _field.`type` match {
              case TType.STRUCT =>
                statistics = _root_.scala.Some(readStatisticsValue(_iprot))
              case _actualType =>
                val _expectedType = TType.STRUCT
                throw new TProtocolException(
                  "Received wrong type for field 'statistics' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 13 =>
            _field.`type` match {
              case TType.LIST =>
                encodingStats = _root_.scala.Some(readEncodingStatsValue(_iprot))
              case _actualType =>
                val _expectedType = TType.LIST
                throw new TProtocolException(
                  "Received wrong type for field 'encodingStats' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case _ =>
            if (_passthroughFields == null)
              _passthroughFields = immutable$Map.newBuilder[Short, TFieldBlob]
            _passthroughFields += (_field.id -> TFieldBlob.read(_field, _iprot))
        }
        _iprot.readFieldEnd()
      }
    }
    _iprot.readStructEnd()

    if (!_got_type) throw new TProtocolException("Required field '`type`' was not found in serialized data for struct ColumnMetaData")
    if (!_got_encodings) throw new TProtocolException("Required field 'encodings' was not found in serialized data for struct ColumnMetaData")
    if (!_got_pathInSchema) throw new TProtocolException("Required field 'pathInSchema' was not found in serialized data for struct ColumnMetaData")
    if (!_got_codec) throw new TProtocolException("Required field 'codec' was not found in serialized data for struct ColumnMetaData")
    if (!_got_numValues) throw new TProtocolException("Required field 'numValues' was not found in serialized data for struct ColumnMetaData")
    if (!_got_totalUncompressedSize) throw new TProtocolException("Required field 'totalUncompressedSize' was not found in serialized data for struct ColumnMetaData")
    if (!_got_totalCompressedSize) throw new TProtocolException("Required field 'totalCompressedSize' was not found in serialized data for struct ColumnMetaData")
    if (!_got_dataPageOffset) throw new TProtocolException("Required field 'dataPageOffset' was not found in serialized data for struct ColumnMetaData")
    new Immutable(
      `type`,
      encodings,
      pathInSchema,
      codec,
      numValues,
      totalUncompressedSize,
      totalCompressedSize,
      keyValueMetadata,
      dataPageOffset,
      indexPageOffset,
      dictionaryPageOffset,
      statistics,
      encodingStats,
      if (_passthroughFields == null)
        NoPassthroughFields
      else
        _passthroughFields.result()
    )
  }

  def apply(
    `type`: parquet.format.Type,
    encodings: Seq[parquet.format.Encoding] = Seq[parquet.format.Encoding](),
    pathInSchema: Seq[String] = Seq[String](),
    codec: parquet.format.CompressionCodec,
    numValues: Long,
    totalUncompressedSize: Long,
    totalCompressedSize: Long,
    keyValueMetadata: _root_.scala.Option[Seq[parquet.format.KeyValue]] = _root_.scala.None,
    dataPageOffset: Long,
    indexPageOffset: _root_.scala.Option[Long] = _root_.scala.None,
    dictionaryPageOffset: _root_.scala.Option[Long] = _root_.scala.None,
    statistics: _root_.scala.Option[parquet.format.Statistics] = _root_.scala.None,
    encodingStats: _root_.scala.Option[Seq[parquet.format.PageEncodingStats]] = _root_.scala.None
  ): ColumnMetaData =
    new Immutable(
      `type`,
      encodings,
      pathInSchema,
      codec,
      numValues,
      totalUncompressedSize,
      totalCompressedSize,
      keyValueMetadata,
      dataPageOffset,
      indexPageOffset,
      dictionaryPageOffset,
      statistics,
      encodingStats
    )

  def unapply(_item: ColumnMetaData): _root_.scala.Option[_root_.scala.Tuple13[parquet.format.Type, Seq[parquet.format.Encoding], Seq[String], parquet.format.CompressionCodec, Long, Long, Long, Option[Seq[parquet.format.KeyValue]], Long, Option[Long], Option[Long], Option[parquet.format.Statistics], Option[Seq[parquet.format.PageEncodingStats]]]] = _root_.scala.Some(_item.toTuple)


  @inline private[format] def readTypeValue(_iprot: TProtocol): parquet.format.Type = {
    parquet.format.Type.getOrUnknown(_iprot.readI32())
  }

  @inline private def writeTypeField(type_item: parquet.format.Type, _oprot: TProtocol): Unit = {
    _oprot.writeFieldBegin(TypeFieldI32)
    writeTypeValue(type_item, _oprot)
    _oprot.writeFieldEnd()
  }

  @inline private def writeTypeValue(type_item: parquet.format.Type, _oprot: TProtocol): Unit = {
    _oprot.writeI32(type_item.value)
  }

  @inline private[format] def readEncodingsValue(_iprot: TProtocol): Seq[parquet.format.Encoding] = {
    val _list = _iprot.readListBegin()
    if (_list.size == 0) {
      _iprot.readListEnd()
      Nil
    } else {
      val _rv = new mutable$ArrayBuffer[parquet.format.Encoding](_list.size)
      var _i = 0
      while (_i < _list.size) {
        _rv += {
          parquet.format.Encoding.getOrUnknown(_iprot.readI32())
        }
        _i += 1
      }
      _iprot.readListEnd()
      _rv
    }
  }

  @inline private def writeEncodingsField(encodings_item: Seq[parquet.format.Encoding], _oprot: TProtocol): Unit = {
    _oprot.writeFieldBegin(EncodingsField)
    writeEncodingsValue(encodings_item, _oprot)
    _oprot.writeFieldEnd()
  }

  @inline private def writeEncodingsValue(encodings_item: Seq[parquet.format.Encoding], _oprot: TProtocol): Unit = {
    _oprot.writeListBegin(new TList(TType.I32, encodings_item.size))
    encodings_item match {
      case _: IndexedSeq[_] =>
        var _i = 0
        val _size = encodings_item.size
        while (_i < _size) {
          val encodings_item_element = encodings_item(_i)
          _oprot.writeI32(encodings_item_element.value)
          _i += 1
        }
      case _ =>
        encodings_item.foreach { encodings_item_element =>
          _oprot.writeI32(encodings_item_element.value)
        }
    }
    _oprot.writeListEnd()
  }

  @inline private[format] def readPathInSchemaValue(_iprot: TProtocol): Seq[String] = {
    val _list = _iprot.readListBegin()
    if (_list.size == 0) {
      _iprot.readListEnd()
      Nil
    } else {
      val _rv = new mutable$ArrayBuffer[String](_list.size)
      var _i = 0
      while (_i < _list.size) {
        _rv += {
          _iprot.readString()
        }
        _i += 1
      }
      _iprot.readListEnd()
      _rv
    }
  }

  @inline private def writePathInSchemaField(pathInSchema_item: Seq[String], _oprot: TProtocol): Unit = {
    _oprot.writeFieldBegin(PathInSchemaField)
    writePathInSchemaValue(pathInSchema_item, _oprot)
    _oprot.writeFieldEnd()
  }

  @inline private def writePathInSchemaValue(pathInSchema_item: Seq[String], _oprot: TProtocol): Unit = {
    _oprot.writeListBegin(new TList(TType.STRING, pathInSchema_item.size))
    pathInSchema_item match {
      case _: IndexedSeq[_] =>
        var _i = 0
        val _size = pathInSchema_item.size
        while (_i < _size) {
          val pathInSchema_item_element = pathInSchema_item(_i)
          _oprot.writeString(pathInSchema_item_element)
          _i += 1
        }
      case _ =>
        pathInSchema_item.foreach { pathInSchema_item_element =>
          _oprot.writeString(pathInSchema_item_element)
        }
    }
    _oprot.writeListEnd()
  }

  @inline private[format] def readCodecValue(_iprot: TProtocol): parquet.format.CompressionCodec = {
    parquet.format.CompressionCodec.getOrUnknown(_iprot.readI32())
  }

  @inline private def writeCodecField(codec_item: parquet.format.CompressionCodec, _oprot: TProtocol): Unit = {
    _oprot.writeFieldBegin(CodecFieldI32)
    writeCodecValue(codec_item, _oprot)
    _oprot.writeFieldEnd()
  }

  @inline private def writeCodecValue(codec_item: parquet.format.CompressionCodec, _oprot: TProtocol): Unit = {
    _oprot.writeI32(codec_item.value)
  }

  @inline private[format] def readNumValuesValue(_iprot: TProtocol): Long = {
    _iprot.readI64()
  }

  @inline private def writeNumValuesField(numValues_item: Long, _oprot: TProtocol): Unit = {
    _oprot.writeFieldBegin(NumValuesField)
    writeNumValuesValue(numValues_item, _oprot)
    _oprot.writeFieldEnd()
  }

  @inline private def writeNumValuesValue(numValues_item: Long, _oprot: TProtocol): Unit = {
    _oprot.writeI64(numValues_item)
  }

  @inline private[format] def readTotalUncompressedSizeValue(_iprot: TProtocol): Long = {
    _iprot.readI64()
  }

  @inline private def writeTotalUncompressedSizeField(totalUncompressedSize_item: Long, _oprot: TProtocol): Unit = {
    _oprot.writeFieldBegin(TotalUncompressedSizeField)
    writeTotalUncompressedSizeValue(totalUncompressedSize_item, _oprot)
    _oprot.writeFieldEnd()
  }

  @inline private def writeTotalUncompressedSizeValue(totalUncompressedSize_item: Long, _oprot: TProtocol): Unit = {
    _oprot.writeI64(totalUncompressedSize_item)
  }

  @inline private[format] def readTotalCompressedSizeValue(_iprot: TProtocol): Long = {
    _iprot.readI64()
  }

  @inline private def writeTotalCompressedSizeField(totalCompressedSize_item: Long, _oprot: TProtocol): Unit = {
    _oprot.writeFieldBegin(TotalCompressedSizeField)
    writeTotalCompressedSizeValue(totalCompressedSize_item, _oprot)
    _oprot.writeFieldEnd()
  }

  @inline private def writeTotalCompressedSizeValue(totalCompressedSize_item: Long, _oprot: TProtocol): Unit = {
    _oprot.writeI64(totalCompressedSize_item)
  }

  @inline private[format] def readKeyValueMetadataValue(_iprot: TProtocol): Seq[parquet.format.KeyValue] = {
    val _list = _iprot.readListBegin()
    if (_list.size == 0) {
      _iprot.readListEnd()
      Nil
    } else {
      val _rv = new mutable$ArrayBuffer[parquet.format.KeyValue](_list.size)
      var _i = 0
      while (_i < _list.size) {
        _rv += {
          parquet.format.KeyValue.decode(_iprot)
        }
        _i += 1
      }
      _iprot.readListEnd()
      _rv
    }
  }

  @inline private def writeKeyValueMetadataField(keyValueMetadata_item: Seq[parquet.format.KeyValue], _oprot: TProtocol): Unit = {
    _oprot.writeFieldBegin(KeyValueMetadataField)
    writeKeyValueMetadataValue(keyValueMetadata_item, _oprot)
    _oprot.writeFieldEnd()
  }

  @inline private def writeKeyValueMetadataValue(keyValueMetadata_item: Seq[parquet.format.KeyValue], _oprot: TProtocol): Unit = {
    _oprot.writeListBegin(new TList(TType.STRUCT, keyValueMetadata_item.size))
    keyValueMetadata_item match {
      case _: IndexedSeq[_] =>
        var _i = 0
        val _size = keyValueMetadata_item.size
        while (_i < _size) {
          val keyValueMetadata_item_element = keyValueMetadata_item(_i)
          keyValueMetadata_item_element.write(_oprot)
          _i += 1
        }
      case _ =>
        keyValueMetadata_item.foreach { keyValueMetadata_item_element =>
          keyValueMetadata_item_element.write(_oprot)
        }
    }
    _oprot.writeListEnd()
  }

  @inline private[format] def readDataPageOffsetValue(_iprot: TProtocol): Long = {
    _iprot.readI64()
  }

  @inline private def writeDataPageOffsetField(dataPageOffset_item: Long, _oprot: TProtocol): Unit = {
    _oprot.writeFieldBegin(DataPageOffsetField)
    writeDataPageOffsetValue(dataPageOffset_item, _oprot)
    _oprot.writeFieldEnd()
  }

  @inline private def writeDataPageOffsetValue(dataPageOffset_item: Long, _oprot: TProtocol): Unit = {
    _oprot.writeI64(dataPageOffset_item)
  }

  @inline private[format] def readIndexPageOffsetValue(_iprot: TProtocol): Long = {
    _iprot.readI64()
  }

  @inline private def writeIndexPageOffsetField(indexPageOffset_item: Long, _oprot: TProtocol): Unit = {
    _oprot.writeFieldBegin(IndexPageOffsetField)
    writeIndexPageOffsetValue(indexPageOffset_item, _oprot)
    _oprot.writeFieldEnd()
  }

  @inline private def writeIndexPageOffsetValue(indexPageOffset_item: Long, _oprot: TProtocol): Unit = {
    _oprot.writeI64(indexPageOffset_item)
  }

  @inline private[format] def readDictionaryPageOffsetValue(_iprot: TProtocol): Long = {
    _iprot.readI64()
  }

  @inline private def writeDictionaryPageOffsetField(dictionaryPageOffset_item: Long, _oprot: TProtocol): Unit = {
    _oprot.writeFieldBegin(DictionaryPageOffsetField)
    writeDictionaryPageOffsetValue(dictionaryPageOffset_item, _oprot)
    _oprot.writeFieldEnd()
  }

  @inline private def writeDictionaryPageOffsetValue(dictionaryPageOffset_item: Long, _oprot: TProtocol): Unit = {
    _oprot.writeI64(dictionaryPageOffset_item)
  }

  @inline private[format] def readStatisticsValue(_iprot: TProtocol): parquet.format.Statistics = {
    parquet.format.Statistics.decode(_iprot)
  }

  @inline private def writeStatisticsField(statistics_item: parquet.format.Statistics, _oprot: TProtocol): Unit = {
    _oprot.writeFieldBegin(StatisticsField)
    writeStatisticsValue(statistics_item, _oprot)
    _oprot.writeFieldEnd()
  }

  @inline private def writeStatisticsValue(statistics_item: parquet.format.Statistics, _oprot: TProtocol): Unit = {
    statistics_item.write(_oprot)
  }

  @inline private[format] def readEncodingStatsValue(_iprot: TProtocol): Seq[parquet.format.PageEncodingStats] = {
    val _list = _iprot.readListBegin()
    if (_list.size == 0) {
      _iprot.readListEnd()
      Nil
    } else {
      val _rv = new mutable$ArrayBuffer[parquet.format.PageEncodingStats](_list.size)
      var _i = 0
      while (_i < _list.size) {
        _rv += {
          parquet.format.PageEncodingStats.decode(_iprot)
        }
        _i += 1
      }
      _iprot.readListEnd()
      _rv
    }
  }

  @inline private def writeEncodingStatsField(encodingStats_item: Seq[parquet.format.PageEncodingStats], _oprot: TProtocol): Unit = {
    _oprot.writeFieldBegin(EncodingStatsField)
    writeEncodingStatsValue(encodingStats_item, _oprot)
    _oprot.writeFieldEnd()
  }

  @inline private def writeEncodingStatsValue(encodingStats_item: Seq[parquet.format.PageEncodingStats], _oprot: TProtocol): Unit = {
    _oprot.writeListBegin(new TList(TType.STRUCT, encodingStats_item.size))
    encodingStats_item match {
      case _: IndexedSeq[_] =>
        var _i = 0
        val _size = encodingStats_item.size
        while (_i < _size) {
          val encodingStats_item_element = encodingStats_item(_i)
          encodingStats_item_element.write(_oprot)
          _i += 1
        }
      case _ =>
        encodingStats_item.foreach { encodingStats_item_element =>
          encodingStats_item_element.write(_oprot)
        }
    }
    _oprot.writeListEnd()
  }


  object Immutable extends ThriftStructCodec3[ColumnMetaData] {
    override def encode(_item: ColumnMetaData, _oproto: TProtocol): Unit = { _item.write(_oproto) }
    override def decode(_iprot: TProtocol): ColumnMetaData = ColumnMetaData.decode(_iprot)
    override lazy val metaData: ThriftStructMetaData[ColumnMetaData] = ColumnMetaData.metaData
  }

  /**
   * The default read-only implementation of ColumnMetaData.  You typically should not need to
   * directly reference this class; instead, use the ColumnMetaData.apply method to construct
   * new instances.
   */
  class Immutable(
      val `type`: parquet.format.Type,
      val encodings: Seq[parquet.format.Encoding],
      val pathInSchema: Seq[String],
      val codec: parquet.format.CompressionCodec,
      val numValues: Long,
      val totalUncompressedSize: Long,
      val totalCompressedSize: Long,
      val keyValueMetadata: _root_.scala.Option[Seq[parquet.format.KeyValue]],
      val dataPageOffset: Long,
      val indexPageOffset: _root_.scala.Option[Long],
      val dictionaryPageOffset: _root_.scala.Option[Long],
      val statistics: _root_.scala.Option[parquet.format.Statistics],
      val encodingStats: _root_.scala.Option[Seq[parquet.format.PageEncodingStats]],
      override val _passthroughFields: immutable$Map[Short, TFieldBlob])
    extends ColumnMetaData {
    def this(
      `type`: parquet.format.Type,
      encodings: Seq[parquet.format.Encoding] = Seq[parquet.format.Encoding](),
      pathInSchema: Seq[String] = Seq[String](),
      codec: parquet.format.CompressionCodec,
      numValues: Long,
      totalUncompressedSize: Long,
      totalCompressedSize: Long,
      keyValueMetadata: _root_.scala.Option[Seq[parquet.format.KeyValue]] = _root_.scala.None,
      dataPageOffset: Long,
      indexPageOffset: _root_.scala.Option[Long] = _root_.scala.None,
      dictionaryPageOffset: _root_.scala.Option[Long] = _root_.scala.None,
      statistics: _root_.scala.Option[parquet.format.Statistics] = _root_.scala.None,
      encodingStats: _root_.scala.Option[Seq[parquet.format.PageEncodingStats]] = _root_.scala.None
    ) = this(
      `type`,
      encodings,
      pathInSchema,
      codec,
      numValues,
      totalUncompressedSize,
      totalCompressedSize,
      keyValueMetadata,
      dataPageOffset,
      indexPageOffset,
      dictionaryPageOffset,
      statistics,
      encodingStats,
      Map.empty
    )
  }

  /**
   * This is another Immutable, this however keeps strings as lazy values that are lazily decoded from the backing
   * array byte on read.
   */
  private[this] class LazyImmutable(
      _proto: LazyTProtocol,
      _buf: Array[Byte],
      _start_offset: Int,
      _end_offset: Int,
      val `type`: parquet.format.Type,
      val encodings: Seq[parquet.format.Encoding],
      val pathInSchema: Seq[String],
      val codec: parquet.format.CompressionCodec,
      val numValues: Long,
      val totalUncompressedSize: Long,
      val totalCompressedSize: Long,
      val keyValueMetadata: _root_.scala.Option[Seq[parquet.format.KeyValue]],
      val dataPageOffset: Long,
      index_page_offsetOffset: Int,
      dictionary_page_offsetOffset: Int,
      val statistics: _root_.scala.Option[parquet.format.Statistics],
      val encodingStats: _root_.scala.Option[Seq[parquet.format.PageEncodingStats]],
      override val _passthroughFields: immutable$Map[Short, TFieldBlob])
    extends ColumnMetaData {

    override def write(_oprot: TProtocol): Unit = {
      _oprot match {
        case i: LazyTProtocol => i.writeRaw(_buf, _start_offset, _end_offset - _start_offset)
        case _ => super.write(_oprot)
      }
    }

    lazy val indexPageOffset: _root_.scala.Option[Long] =
      if (index_page_offsetOffset == -1)
        None
      else {
        Some(_proto.decodeI64(_buf, index_page_offsetOffset))
      }
    lazy val dictionaryPageOffset: _root_.scala.Option[Long] =
      if (dictionary_page_offsetOffset == -1)
        None
      else {
        Some(_proto.decodeI64(_buf, dictionary_page_offsetOffset))
      }

    /**
     * Override the super hash code to make it a lazy val rather than def.
     *
     * Calculating the hash code can be expensive, caching it where possible
     * can provide significant performance wins. (Key in a hash map for instance)
     * Usually not safe since the normal constructor will accept a mutable map or
     * set as an arg
     * Here however we control how the class is generated from serialized data.
     * With the class private and the contract that we throw away our mutable references
     * having the hash code lazy here is safe.
     */
    override lazy val hashCode = super.hashCode
  }

  /**
   * This Proxy trait allows you to extend the ColumnMetaData trait with additional state or
   * behavior and implement the read-only methods from ColumnMetaData using an underlying
   * instance.
   */
  trait Proxy extends ColumnMetaData {
    protected def _underlying_ColumnMetaData: ColumnMetaData
    override def `type`: parquet.format.Type = _underlying_ColumnMetaData.`type`
    override def encodings: Seq[parquet.format.Encoding] = _underlying_ColumnMetaData.encodings
    override def pathInSchema: Seq[String] = _underlying_ColumnMetaData.pathInSchema
    override def codec: parquet.format.CompressionCodec = _underlying_ColumnMetaData.codec
    override def numValues: Long = _underlying_ColumnMetaData.numValues
    override def totalUncompressedSize: Long = _underlying_ColumnMetaData.totalUncompressedSize
    override def totalCompressedSize: Long = _underlying_ColumnMetaData.totalCompressedSize
    override def keyValueMetadata: _root_.scala.Option[Seq[parquet.format.KeyValue]] = _underlying_ColumnMetaData.keyValueMetadata
    override def dataPageOffset: Long = _underlying_ColumnMetaData.dataPageOffset
    override def indexPageOffset: _root_.scala.Option[Long] = _underlying_ColumnMetaData.indexPageOffset
    override def dictionaryPageOffset: _root_.scala.Option[Long] = _underlying_ColumnMetaData.dictionaryPageOffset
    override def statistics: _root_.scala.Option[parquet.format.Statistics] = _underlying_ColumnMetaData.statistics
    override def encodingStats: _root_.scala.Option[Seq[parquet.format.PageEncodingStats]] = _underlying_ColumnMetaData.encodingStats
    override def _passthroughFields = _underlying_ColumnMetaData._passthroughFields
  }
}

/**
 * Prefer the companion object's [[parquet.format.ColumnMetaData.apply]]
 * for construction if you don't need to specify passthrough fields.
 */
trait ColumnMetaData
  extends ThriftStruct
  with _root_.scala.Product13[parquet.format.Type, Seq[parquet.format.Encoding], Seq[String], parquet.format.CompressionCodec, Long, Long, Long, Option[Seq[parquet.format.KeyValue]], Long, Option[Long], Option[Long], Option[parquet.format.Statistics], Option[Seq[parquet.format.PageEncodingStats]]]
  with HasThriftStructCodec3[ColumnMetaData]
  with java.io.Serializable
{
  import ColumnMetaData._

  def `type`: parquet.format.Type
  def encodings: Seq[parquet.format.Encoding]
  def pathInSchema: Seq[String]
  def codec: parquet.format.CompressionCodec
  def numValues: Long
  def totalUncompressedSize: Long
  def totalCompressedSize: Long
  def keyValueMetadata: _root_.scala.Option[Seq[parquet.format.KeyValue]]
  def dataPageOffset: Long
  def indexPageOffset: _root_.scala.Option[Long]
  def dictionaryPageOffset: _root_.scala.Option[Long]
  def statistics: _root_.scala.Option[parquet.format.Statistics]
  def encodingStats: _root_.scala.Option[Seq[parquet.format.PageEncodingStats]]

  def _passthroughFields: immutable$Map[Short, TFieldBlob] = immutable$Map.empty

  def _1 = `type`
  def _2 = encodings
  def _3 = pathInSchema
  def _4 = codec
  def _5 = numValues
  def _6 = totalUncompressedSize
  def _7 = totalCompressedSize
  def _8 = keyValueMetadata
  def _9 = dataPageOffset
  def _10 = indexPageOffset
  def _11 = dictionaryPageOffset
  def _12 = statistics
  def _13 = encodingStats

  def toTuple: _root_.scala.Tuple13[parquet.format.Type, Seq[parquet.format.Encoding], Seq[String], parquet.format.CompressionCodec, Long, Long, Long, Option[Seq[parquet.format.KeyValue]], Long, Option[Long], Option[Long], Option[parquet.format.Statistics], Option[Seq[parquet.format.PageEncodingStats]]] = {
    (
      `type`,
      encodings,
      pathInSchema,
      codec,
      numValues,
      totalUncompressedSize,
      totalCompressedSize,
      keyValueMetadata,
      dataPageOffset,
      indexPageOffset,
      dictionaryPageOffset,
      statistics,
      encodingStats
    )
  }


  /**
   * Gets a field value encoded as a binary blob using TCompactProtocol.  If the specified field
   * is present in the passthrough map, that value is returned.  Otherwise, if the specified field
   * is known and not optional and set to None, then the field is serialized and returned.
   */
  def getFieldBlob(_fieldId: Short): _root_.scala.Option[TFieldBlob] = {
    lazy val _buff = new TMemoryBuffer(32)
    lazy val _oprot = new TCompactProtocol(_buff)
    _passthroughFields.get(_fieldId) match {
      case blob: _root_.scala.Some[TFieldBlob] => blob
      case _root_.scala.None => {
        val _fieldOpt: _root_.scala.Option[TField] =
          _fieldId match {
            case 1 =>
              if (`type` ne null) {
                writeTypeValue(`type`, _oprot)
                _root_.scala.Some(ColumnMetaData.TypeField)
              } else {
                _root_.scala.None
              }
            case 2 =>
              if (encodings ne null) {
                writeEncodingsValue(encodings, _oprot)
                _root_.scala.Some(ColumnMetaData.EncodingsField)
              } else {
                _root_.scala.None
              }
            case 3 =>
              if (pathInSchema ne null) {
                writePathInSchemaValue(pathInSchema, _oprot)
                _root_.scala.Some(ColumnMetaData.PathInSchemaField)
              } else {
                _root_.scala.None
              }
            case 4 =>
              if (codec ne null) {
                writeCodecValue(codec, _oprot)
                _root_.scala.Some(ColumnMetaData.CodecField)
              } else {
                _root_.scala.None
              }
            case 5 =>
              if (true) {
                writeNumValuesValue(numValues, _oprot)
                _root_.scala.Some(ColumnMetaData.NumValuesField)
              } else {
                _root_.scala.None
              }
            case 6 =>
              if (true) {
                writeTotalUncompressedSizeValue(totalUncompressedSize, _oprot)
                _root_.scala.Some(ColumnMetaData.TotalUncompressedSizeField)
              } else {
                _root_.scala.None
              }
            case 7 =>
              if (true) {
                writeTotalCompressedSizeValue(totalCompressedSize, _oprot)
                _root_.scala.Some(ColumnMetaData.TotalCompressedSizeField)
              } else {
                _root_.scala.None
              }
            case 8 =>
              if (keyValueMetadata.isDefined) {
                writeKeyValueMetadataValue(keyValueMetadata.get, _oprot)
                _root_.scala.Some(ColumnMetaData.KeyValueMetadataField)
              } else {
                _root_.scala.None
              }
            case 9 =>
              if (true) {
                writeDataPageOffsetValue(dataPageOffset, _oprot)
                _root_.scala.Some(ColumnMetaData.DataPageOffsetField)
              } else {
                _root_.scala.None
              }
            case 10 =>
              if (indexPageOffset.isDefined) {
                writeIndexPageOffsetValue(indexPageOffset.get, _oprot)
                _root_.scala.Some(ColumnMetaData.IndexPageOffsetField)
              } else {
                _root_.scala.None
              }
            case 11 =>
              if (dictionaryPageOffset.isDefined) {
                writeDictionaryPageOffsetValue(dictionaryPageOffset.get, _oprot)
                _root_.scala.Some(ColumnMetaData.DictionaryPageOffsetField)
              } else {
                _root_.scala.None
              }
            case 12 =>
              if (statistics.isDefined) {
                writeStatisticsValue(statistics.get, _oprot)
                _root_.scala.Some(ColumnMetaData.StatisticsField)
              } else {
                _root_.scala.None
              }
            case 13 =>
              if (encodingStats.isDefined) {
                writeEncodingStatsValue(encodingStats.get, _oprot)
                _root_.scala.Some(ColumnMetaData.EncodingStatsField)
              } else {
                _root_.scala.None
              }
            case _ => _root_.scala.None
          }
        _fieldOpt match {
          case _root_.scala.Some(_field) =>
            _root_.scala.Some(TFieldBlob(_field, Buf.ByteArray.Owned(_buff.getArray())))
          case _root_.scala.None =>
            _root_.scala.None
        }
      }
    }
  }

  /**
   * Collects TCompactProtocol-encoded field values according to `getFieldBlob` into a map.
   */
  def getFieldBlobs(ids: TraversableOnce[Short]): immutable$Map[Short, TFieldBlob] =
    (ids flatMap { id => getFieldBlob(id) map { id -> _ } }).toMap

  /**
   * Sets a field using a TCompactProtocol-encoded binary blob.  If the field is a known
   * field, the blob is decoded and the field is set to the decoded value.  If the field
   * is unknown and passthrough fields are enabled, then the blob will be stored in
   * _passthroughFields.
   */
  def setField(_blob: TFieldBlob): ColumnMetaData = {
    var `type`: parquet.format.Type = this.`type`
    var encodings: Seq[parquet.format.Encoding] = this.encodings
    var pathInSchema: Seq[String] = this.pathInSchema
    var codec: parquet.format.CompressionCodec = this.codec
    var numValues: Long = this.numValues
    var totalUncompressedSize: Long = this.totalUncompressedSize
    var totalCompressedSize: Long = this.totalCompressedSize
    var keyValueMetadata: _root_.scala.Option[Seq[parquet.format.KeyValue]] = this.keyValueMetadata
    var dataPageOffset: Long = this.dataPageOffset
    var indexPageOffset: _root_.scala.Option[Long] = this.indexPageOffset
    var dictionaryPageOffset: _root_.scala.Option[Long] = this.dictionaryPageOffset
    var statistics: _root_.scala.Option[parquet.format.Statistics] = this.statistics
    var encodingStats: _root_.scala.Option[Seq[parquet.format.PageEncodingStats]] = this.encodingStats
    var _passthroughFields = this._passthroughFields
    _blob.id match {
      case 1 =>
        `type` = readTypeValue(_blob.read)
      case 2 =>
        encodings = readEncodingsValue(_blob.read)
      case 3 =>
        pathInSchema = readPathInSchemaValue(_blob.read)
      case 4 =>
        codec = readCodecValue(_blob.read)
      case 5 =>
        numValues = readNumValuesValue(_blob.read)
      case 6 =>
        totalUncompressedSize = readTotalUncompressedSizeValue(_blob.read)
      case 7 =>
        totalCompressedSize = readTotalCompressedSizeValue(_blob.read)
      case 8 =>
        keyValueMetadata = _root_.scala.Some(readKeyValueMetadataValue(_blob.read))
      case 9 =>
        dataPageOffset = readDataPageOffsetValue(_blob.read)
      case 10 =>
        indexPageOffset = _root_.scala.Some(readIndexPageOffsetValue(_blob.read))
      case 11 =>
        dictionaryPageOffset = _root_.scala.Some(readDictionaryPageOffsetValue(_blob.read))
      case 12 =>
        statistics = _root_.scala.Some(readStatisticsValue(_blob.read))
      case 13 =>
        encodingStats = _root_.scala.Some(readEncodingStatsValue(_blob.read))
      case _ => _passthroughFields += (_blob.id -> _blob)
    }
    new Immutable(
      `type`,
      encodings,
      pathInSchema,
      codec,
      numValues,
      totalUncompressedSize,
      totalCompressedSize,
      keyValueMetadata,
      dataPageOffset,
      indexPageOffset,
      dictionaryPageOffset,
      statistics,
      encodingStats,
      _passthroughFields
    )
  }

  /**
   * If the specified field is optional, it is set to None.  Otherwise, if the field is
   * known, it is reverted to its default value; if the field is unknown, it is removed
   * from the passthroughFields map, if present.
   */
  def unsetField(_fieldId: Short): ColumnMetaData = {
    var `type`: parquet.format.Type = this.`type`
    var encodings: Seq[parquet.format.Encoding] = this.encodings
    var pathInSchema: Seq[String] = this.pathInSchema
    var codec: parquet.format.CompressionCodec = this.codec
    var numValues: Long = this.numValues
    var totalUncompressedSize: Long = this.totalUncompressedSize
    var totalCompressedSize: Long = this.totalCompressedSize
    var keyValueMetadata: _root_.scala.Option[Seq[parquet.format.KeyValue]] = this.keyValueMetadata
    var dataPageOffset: Long = this.dataPageOffset
    var indexPageOffset: _root_.scala.Option[Long] = this.indexPageOffset
    var dictionaryPageOffset: _root_.scala.Option[Long] = this.dictionaryPageOffset
    var statistics: _root_.scala.Option[parquet.format.Statistics] = this.statistics
    var encodingStats: _root_.scala.Option[Seq[parquet.format.PageEncodingStats]] = this.encodingStats

    _fieldId match {
      case 1 =>
        `type` = null
      case 2 =>
        encodings = Seq[parquet.format.Encoding]()
      case 3 =>
        pathInSchema = Seq[String]()
      case 4 =>
        codec = null
      case 5 =>
        numValues = 0L
      case 6 =>
        totalUncompressedSize = 0L
      case 7 =>
        totalCompressedSize = 0L
      case 8 =>
        keyValueMetadata = _root_.scala.None
      case 9 =>
        dataPageOffset = 0L
      case 10 =>
        indexPageOffset = _root_.scala.None
      case 11 =>
        dictionaryPageOffset = _root_.scala.None
      case 12 =>
        statistics = _root_.scala.None
      case 13 =>
        encodingStats = _root_.scala.None
      case _ =>
    }
    new Immutable(
      `type`,
      encodings,
      pathInSchema,
      codec,
      numValues,
      totalUncompressedSize,
      totalCompressedSize,
      keyValueMetadata,
      dataPageOffset,
      indexPageOffset,
      dictionaryPageOffset,
      statistics,
      encodingStats,
      _passthroughFields - _fieldId
    )
  }

  /**
   * If the specified field is optional, it is set to None.  Otherwise, if the field is
   * known, it is reverted to its default value; if the field is unknown, it is removed
   * from the passthroughFields map, if present.
   */
  def unsetType: ColumnMetaData = unsetField(1)

  def unsetEncodings: ColumnMetaData = unsetField(2)

  def unsetPathInSchema: ColumnMetaData = unsetField(3)

  def unsetCodec: ColumnMetaData = unsetField(4)

  def unsetNumValues: ColumnMetaData = unsetField(5)

  def unsetTotalUncompressedSize: ColumnMetaData = unsetField(6)

  def unsetTotalCompressedSize: ColumnMetaData = unsetField(7)

  def unsetKeyValueMetadata: ColumnMetaData = unsetField(8)

  def unsetDataPageOffset: ColumnMetaData = unsetField(9)

  def unsetIndexPageOffset: ColumnMetaData = unsetField(10)

  def unsetDictionaryPageOffset: ColumnMetaData = unsetField(11)

  def unsetStatistics: ColumnMetaData = unsetField(12)

  def unsetEncodingStats: ColumnMetaData = unsetField(13)


  override def write(_oprot: TProtocol): Unit = {
    ColumnMetaData.validate(this)
    _oprot.writeStructBegin(Struct)
    if (`type` ne null) writeTypeField(`type`, _oprot)
    if (encodings ne null) writeEncodingsField(encodings, _oprot)
    if (pathInSchema ne null) writePathInSchemaField(pathInSchema, _oprot)
    if (codec ne null) writeCodecField(codec, _oprot)
    writeNumValuesField(numValues, _oprot)
    writeTotalUncompressedSizeField(totalUncompressedSize, _oprot)
    writeTotalCompressedSizeField(totalCompressedSize, _oprot)
    if (keyValueMetadata.isDefined) writeKeyValueMetadataField(keyValueMetadata.get, _oprot)
    writeDataPageOffsetField(dataPageOffset, _oprot)
    if (indexPageOffset.isDefined) writeIndexPageOffsetField(indexPageOffset.get, _oprot)
    if (dictionaryPageOffset.isDefined) writeDictionaryPageOffsetField(dictionaryPageOffset.get, _oprot)
    if (statistics.isDefined) writeStatisticsField(statistics.get, _oprot)
    if (encodingStats.isDefined) writeEncodingStatsField(encodingStats.get, _oprot)
    if (_passthroughFields.nonEmpty) {
      _passthroughFields.values.foreach { _.write(_oprot) }
    }
    _oprot.writeFieldStop()
    _oprot.writeStructEnd()
  }

  def copy(
    `type`: parquet.format.Type = this.`type`,
    encodings: Seq[parquet.format.Encoding] = this.encodings,
    pathInSchema: Seq[String] = this.pathInSchema,
    codec: parquet.format.CompressionCodec = this.codec,
    numValues: Long = this.numValues,
    totalUncompressedSize: Long = this.totalUncompressedSize,
    totalCompressedSize: Long = this.totalCompressedSize,
    keyValueMetadata: _root_.scala.Option[Seq[parquet.format.KeyValue]] = this.keyValueMetadata,
    dataPageOffset: Long = this.dataPageOffset,
    indexPageOffset: _root_.scala.Option[Long] = this.indexPageOffset,
    dictionaryPageOffset: _root_.scala.Option[Long] = this.dictionaryPageOffset,
    statistics: _root_.scala.Option[parquet.format.Statistics] = this.statistics,
    encodingStats: _root_.scala.Option[Seq[parquet.format.PageEncodingStats]] = this.encodingStats,
    _passthroughFields: immutable$Map[Short, TFieldBlob] = this._passthroughFields
  ): ColumnMetaData =
    new Immutable(
      `type`,
      encodings,
      pathInSchema,
      codec,
      numValues,
      totalUncompressedSize,
      totalCompressedSize,
      keyValueMetadata,
      dataPageOffset,
      indexPageOffset,
      dictionaryPageOffset,
      statistics,
      encodingStats,
      _passthroughFields
    )

  override def canEqual(other: Any): Boolean = other.isInstanceOf[ColumnMetaData]

  private def _equals(x: ColumnMetaData, y: ColumnMetaData): Boolean =
      x.productArity == y.productArity &&
      x.productIterator.sameElements(y.productIterator)

  override def equals(other: Any): Boolean =
    canEqual(other) &&
      _equals(this, other.asInstanceOf[ColumnMetaData]) &&
      _passthroughFields == other.asInstanceOf[ColumnMetaData]._passthroughFields

  override def hashCode: Int = _root_.scala.runtime.ScalaRunTime._hashCode(this)

  override def toString: String = _root_.scala.runtime.ScalaRunTime._toString(this)


  override def productArity: Int = 13

  override def productElement(n: Int): Any = n match {
    case 0 => this.`type`
    case 1 => this.encodings
    case 2 => this.pathInSchema
    case 3 => this.codec
    case 4 => this.numValues
    case 5 => this.totalUncompressedSize
    case 6 => this.totalCompressedSize
    case 7 => this.keyValueMetadata
    case 8 => this.dataPageOffset
    case 9 => this.indexPageOffset
    case 10 => this.dictionaryPageOffset
    case 11 => this.statistics
    case 12 => this.encodingStats
    case _ => throw new IndexOutOfBoundsException(n.toString)
  }

  override def productPrefix: String = "ColumnMetaData"

  def _codec: ThriftStructCodec3[ColumnMetaData] = ColumnMetaData
}

