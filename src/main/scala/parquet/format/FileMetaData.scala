/**
 * Generated by Scrooge
 *   version: 4.16.0-SNAPSHOT
 *   rev: eb110c820bcd2734b26023f24d636bf6d37511b3
 *   built at: 20170607-185808
 */
package parquet.format

import com.twitter.io.Buf
import com.twitter.scrooge.{
  HasThriftStructCodec3,
  LazyTProtocol,
  TFieldBlob,
  ThriftException,
  ThriftStruct,
  ThriftStructCodec3,
  ThriftStructFieldInfo,
  ThriftStructMetaData,
  ThriftUtil
}
import org.apache.thrift.protocol._
import org.apache.thrift.transport.{TMemoryBuffer, TTransport, TIOStreamTransport}
import java.io.ByteArrayInputStream
import java.nio.ByteBuffer
import java.util.Arrays
import java.util.concurrent.atomic.AtomicInteger
import scala.collection.immutable.{Map => immutable$Map}
import scala.collection.mutable.Builder
import scala.collection.mutable.{
  ArrayBuffer => mutable$ArrayBuffer, Buffer => mutable$Buffer,
  HashMap => mutable$HashMap, HashSet => mutable$HashSet}
import scala.collection.{Map, Set}

/**
 * Description for file metadata
 */
object FileMetaData extends ThriftStructCodec3[FileMetaData] {
  val NoPassthroughFields: immutable$Map[Short, TFieldBlob] = immutable$Map.empty[Short, TFieldBlob]
  val Struct = new TStruct("FileMetaData")
  val VersionField = new TField("version", TType.I32, 1)
  val VersionFieldManifest = implicitly[Manifest[Int]]
  val SchemaField = new TField("schema", TType.LIST, 2)
  val SchemaFieldManifest = implicitly[Manifest[Seq[parquet.format.SchemaElement]]]
  val NumRowsField = new TField("num_rows", TType.I64, 3)
  val NumRowsFieldManifest = implicitly[Manifest[Long]]
  val RowGroupsField = new TField("row_groups", TType.LIST, 4)
  val RowGroupsFieldManifest = implicitly[Manifest[Seq[parquet.format.RowGroup]]]
  val KeyValueMetadataField = new TField("key_value_metadata", TType.LIST, 5)
  val KeyValueMetadataFieldManifest = implicitly[Manifest[Seq[parquet.format.KeyValue]]]
  val CreatedByField = new TField("created_by", TType.STRING, 6)
  val CreatedByFieldManifest = implicitly[Manifest[String]]

  /**
   * Field information in declaration order.
   */
  lazy val fieldInfos: scala.List[ThriftStructFieldInfo] = scala.List[ThriftStructFieldInfo](
    new ThriftStructFieldInfo(
      VersionField,
      false,
      true,
      VersionFieldManifest,
      _root_.scala.None,
      _root_.scala.None,
      immutable$Map.empty[String, String],
      immutable$Map.empty[String, String],
      None
    ),
    new ThriftStructFieldInfo(
      SchemaField,
      false,
      true,
      SchemaFieldManifest,
      _root_.scala.None,
      _root_.scala.Some(implicitly[Manifest[parquet.format.SchemaElement]]),
      immutable$Map.empty[String, String],
      immutable$Map.empty[String, String],
      None
    ),
    new ThriftStructFieldInfo(
      NumRowsField,
      false,
      true,
      NumRowsFieldManifest,
      _root_.scala.None,
      _root_.scala.None,
      immutable$Map.empty[String, String],
      immutable$Map.empty[String, String],
      None
    ),
    new ThriftStructFieldInfo(
      RowGroupsField,
      false,
      true,
      RowGroupsFieldManifest,
      _root_.scala.None,
      _root_.scala.Some(implicitly[Manifest[parquet.format.RowGroup]]),
      immutable$Map.empty[String, String],
      immutable$Map.empty[String, String],
      None
    ),
    new ThriftStructFieldInfo(
      KeyValueMetadataField,
      true,
      false,
      KeyValueMetadataFieldManifest,
      _root_.scala.None,
      _root_.scala.Some(implicitly[Manifest[parquet.format.KeyValue]]),
      immutable$Map.empty[String, String],
      immutable$Map.empty[String, String],
      None
    ),
    new ThriftStructFieldInfo(
      CreatedByField,
      true,
      false,
      CreatedByFieldManifest,
      _root_.scala.None,
      _root_.scala.None,
      immutable$Map.empty[String, String],
      immutable$Map.empty[String, String],
      None
    )
  )

  lazy val structAnnotations: immutable$Map[String, String] =
    immutable$Map.empty[String, String]

  /**
   * Checks that all required fields are non-null.
   */
  def validate(_item: FileMetaData): Unit = {
    if (_item.schema == null) throw new TProtocolException("Required field schema cannot be null")
    if (_item.rowGroups == null) throw new TProtocolException("Required field rowGroups cannot be null")
  }

  def withoutPassthroughFields(original: FileMetaData): FileMetaData =
    new Immutable(
      version =
        {
          val field = original.version
          field
        },
      schema =
        {
          val field = original.schema
          field.map { field =>
            parquet.format.SchemaElement.withoutPassthroughFields(field)
          }
        },
      numRows =
        {
          val field = original.numRows
          field
        },
      rowGroups =
        {
          val field = original.rowGroups
          field.map { field =>
            parquet.format.RowGroup.withoutPassthroughFields(field)
          }
        },
      keyValueMetadata =
        {
          val field = original.keyValueMetadata
          field.map { field =>
            field.map { field =>
              parquet.format.KeyValue.withoutPassthroughFields(field)
            }
          }
        },
      createdBy =
        {
          val field = original.createdBy
          field.map { field =>
            field
          }
        }
    )

  override def encode(_item: FileMetaData, _oproto: TProtocol): Unit = {
    _item.write(_oproto)
  }


  private[this] def lazyDecode(_iprot: LazyTProtocol): FileMetaData = {

    var version: Int = 0
    var _got_version = false
    var schema: Seq[parquet.format.SchemaElement] = Seq[parquet.format.SchemaElement]()
    var _got_schema = false
    var numRows: Long = 0L
    var _got_numRows = false
    var rowGroups: Seq[parquet.format.RowGroup] = Seq[parquet.format.RowGroup]()
    var _got_rowGroups = false
    var keyValueMetadata: Option[Seq[parquet.format.KeyValue]] = None
    var created_byOffset: Int = -1

    var _passthroughFields: Builder[(Short, TFieldBlob), immutable$Map[Short, TFieldBlob]] = null
    var _done = false
    val _start_offset = _iprot.offset

    _iprot.readStructBegin()
    while (!_done) {
      val _field = _iprot.readFieldBegin()
      if (_field.`type` == TType.STOP) {
        _done = true
      } else {
        _field.id match {
          case 1 =>
            _field.`type` match {
              case TType.I32 =>
    
                version = readVersionValue(_iprot)
                _got_version = true
              case _actualType =>
                val _expectedType = TType.I32
                throw new TProtocolException(
                  "Received wrong type for field 'version' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 2 =>
            _field.`type` match {
              case TType.LIST =>
    
                schema = readSchemaValue(_iprot)
                _got_schema = true
              case _actualType =>
                val _expectedType = TType.LIST
                throw new TProtocolException(
                  "Received wrong type for field 'schema' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 3 =>
            _field.`type` match {
              case TType.I64 =>
    
                numRows = readNumRowsValue(_iprot)
                _got_numRows = true
              case _actualType =>
                val _expectedType = TType.I64
                throw new TProtocolException(
                  "Received wrong type for field 'numRows' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 4 =>
            _field.`type` match {
              case TType.LIST =>
    
                rowGroups = readRowGroupsValue(_iprot)
                _got_rowGroups = true
              case _actualType =>
                val _expectedType = TType.LIST
                throw new TProtocolException(
                  "Received wrong type for field 'rowGroups' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 5 =>
            _field.`type` match {
              case TType.LIST =>
    
                keyValueMetadata = Some(readKeyValueMetadataValue(_iprot))
              case _actualType =>
                val _expectedType = TType.LIST
                throw new TProtocolException(
                  "Received wrong type for field 'keyValueMetadata' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 6 =>
            _field.`type` match {
              case TType.STRING =>
                created_byOffset = _iprot.offsetSkipString
    
              case _actualType =>
                val _expectedType = TType.STRING
                throw new TProtocolException(
                  "Received wrong type for field 'createdBy' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case _ =>
            if (_passthroughFields == null)
              _passthroughFields = immutable$Map.newBuilder[Short, TFieldBlob]
            _passthroughFields += (_field.id -> TFieldBlob.read(_field, _iprot))
        }
        _iprot.readFieldEnd()
      }
    }
    _iprot.readStructEnd()

    if (!_got_version) throw new TProtocolException("Required field 'version' was not found in serialized data for struct FileMetaData")
    if (!_got_schema) throw new TProtocolException("Required field 'schema' was not found in serialized data for struct FileMetaData")
    if (!_got_numRows) throw new TProtocolException("Required field 'numRows' was not found in serialized data for struct FileMetaData")
    if (!_got_rowGroups) throw new TProtocolException("Required field 'rowGroups' was not found in serialized data for struct FileMetaData")
    new LazyImmutable(
      _iprot,
      _iprot.buffer,
      _start_offset,
      _iprot.offset,
      version,
      schema,
      numRows,
      rowGroups,
      keyValueMetadata,
      created_byOffset,
      if (_passthroughFields == null)
        NoPassthroughFields
      else
        _passthroughFields.result()
    )
  }

  override def decode(_iprot: TProtocol): FileMetaData =
    _iprot match {
      case i: LazyTProtocol => lazyDecode(i)
      case i => eagerDecode(i)
    }

  private[format] def eagerDecode(_iprot: TProtocol): FileMetaData = {
    var version: Int = 0
    var _got_version = false
    var schema: Seq[parquet.format.SchemaElement] = Seq[parquet.format.SchemaElement]()
    var _got_schema = false
    var numRows: Long = 0L
    var _got_numRows = false
    var rowGroups: Seq[parquet.format.RowGroup] = Seq[parquet.format.RowGroup]()
    var _got_rowGroups = false
    var keyValueMetadata: _root_.scala.Option[Seq[parquet.format.KeyValue]] = _root_.scala.None
    var createdBy: _root_.scala.Option[String] = _root_.scala.None
    var _passthroughFields: Builder[(Short, TFieldBlob), immutable$Map[Short, TFieldBlob]] = null
    var _done = false

    _iprot.readStructBegin()
    while (!_done) {
      val _field = _iprot.readFieldBegin()
      if (_field.`type` == TType.STOP) {
        _done = true
      } else {
        _field.id match {
          case 1 =>
            _field.`type` match {
              case TType.I32 =>
                version = readVersionValue(_iprot)
                _got_version = true
              case _actualType =>
                val _expectedType = TType.I32
                throw new TProtocolException(
                  "Received wrong type for field 'version' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 2 =>
            _field.`type` match {
              case TType.LIST =>
                schema = readSchemaValue(_iprot)
                _got_schema = true
              case _actualType =>
                val _expectedType = TType.LIST
                throw new TProtocolException(
                  "Received wrong type for field 'schema' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 3 =>
            _field.`type` match {
              case TType.I64 =>
                numRows = readNumRowsValue(_iprot)
                _got_numRows = true
              case _actualType =>
                val _expectedType = TType.I64
                throw new TProtocolException(
                  "Received wrong type for field 'numRows' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 4 =>
            _field.`type` match {
              case TType.LIST =>
                rowGroups = readRowGroupsValue(_iprot)
                _got_rowGroups = true
              case _actualType =>
                val _expectedType = TType.LIST
                throw new TProtocolException(
                  "Received wrong type for field 'rowGroups' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 5 =>
            _field.`type` match {
              case TType.LIST =>
                keyValueMetadata = _root_.scala.Some(readKeyValueMetadataValue(_iprot))
              case _actualType =>
                val _expectedType = TType.LIST
                throw new TProtocolException(
                  "Received wrong type for field 'keyValueMetadata' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 6 =>
            _field.`type` match {
              case TType.STRING =>
                createdBy = _root_.scala.Some(readCreatedByValue(_iprot))
              case _actualType =>
                val _expectedType = TType.STRING
                throw new TProtocolException(
                  "Received wrong type for field 'createdBy' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case _ =>
            if (_passthroughFields == null)
              _passthroughFields = immutable$Map.newBuilder[Short, TFieldBlob]
            _passthroughFields += (_field.id -> TFieldBlob.read(_field, _iprot))
        }
        _iprot.readFieldEnd()
      }
    }
    _iprot.readStructEnd()

    if (!_got_version) throw new TProtocolException("Required field 'version' was not found in serialized data for struct FileMetaData")
    if (!_got_schema) throw new TProtocolException("Required field 'schema' was not found in serialized data for struct FileMetaData")
    if (!_got_numRows) throw new TProtocolException("Required field 'numRows' was not found in serialized data for struct FileMetaData")
    if (!_got_rowGroups) throw new TProtocolException("Required field 'rowGroups' was not found in serialized data for struct FileMetaData")
    new Immutable(
      version,
      schema,
      numRows,
      rowGroups,
      keyValueMetadata,
      createdBy,
      if (_passthroughFields == null)
        NoPassthroughFields
      else
        _passthroughFields.result()
    )
  }

  def apply(
    version: Int,
    schema: Seq[parquet.format.SchemaElement] = Seq[parquet.format.SchemaElement](),
    numRows: Long,
    rowGroups: Seq[parquet.format.RowGroup] = Seq[parquet.format.RowGroup](),
    keyValueMetadata: _root_.scala.Option[Seq[parquet.format.KeyValue]] = _root_.scala.None,
    createdBy: _root_.scala.Option[String] = _root_.scala.None
  ): FileMetaData =
    new Immutable(
      version,
      schema,
      numRows,
      rowGroups,
      keyValueMetadata,
      createdBy
    )

  def unapply(_item: FileMetaData): _root_.scala.Option[_root_.scala.Tuple6[Int, Seq[parquet.format.SchemaElement], Long, Seq[parquet.format.RowGroup], Option[Seq[parquet.format.KeyValue]], Option[String]]] = _root_.scala.Some(_item.toTuple)


  @inline private[format] def readVersionValue(_iprot: TProtocol): Int = {
    _iprot.readI32()
  }

  @inline private def writeVersionField(version_item: Int, _oprot: TProtocol): Unit = {
    _oprot.writeFieldBegin(VersionField)
    writeVersionValue(version_item, _oprot)
    _oprot.writeFieldEnd()
  }

  @inline private def writeVersionValue(version_item: Int, _oprot: TProtocol): Unit = {
    _oprot.writeI32(version_item)
  }

  @inline private[format] def readSchemaValue(_iprot: TProtocol): Seq[parquet.format.SchemaElement] = {
    val _list = _iprot.readListBegin()
    if (_list.size == 0) {
      _iprot.readListEnd()
      Nil
    } else {
      val _rv = new mutable$ArrayBuffer[parquet.format.SchemaElement](_list.size)
      var _i = 0
      while (_i < _list.size) {
        _rv += {
          parquet.format.SchemaElement.decode(_iprot)
        }
        _i += 1
      }
      _iprot.readListEnd()
      _rv
    }
  }

  @inline private def writeSchemaField(schema_item: Seq[parquet.format.SchemaElement], _oprot: TProtocol): Unit = {
    _oprot.writeFieldBegin(SchemaField)
    writeSchemaValue(schema_item, _oprot)
    _oprot.writeFieldEnd()
  }

  @inline private def writeSchemaValue(schema_item: Seq[parquet.format.SchemaElement], _oprot: TProtocol): Unit = {
    _oprot.writeListBegin(new TList(TType.STRUCT, schema_item.size))
    schema_item match {
      case _: IndexedSeq[_] =>
        var _i = 0
        val _size = schema_item.size
        while (_i < _size) {
          val schema_item_element = schema_item(_i)
          schema_item_element.write(_oprot)
          _i += 1
        }
      case _ =>
        schema_item.foreach { schema_item_element =>
          schema_item_element.write(_oprot)
        }
    }
    _oprot.writeListEnd()
  }

  @inline private[format] def readNumRowsValue(_iprot: TProtocol): Long = {
    _iprot.readI64()
  }

  @inline private def writeNumRowsField(numRows_item: Long, _oprot: TProtocol): Unit = {
    _oprot.writeFieldBegin(NumRowsField)
    writeNumRowsValue(numRows_item, _oprot)
    _oprot.writeFieldEnd()
  }

  @inline private def writeNumRowsValue(numRows_item: Long, _oprot: TProtocol): Unit = {
    _oprot.writeI64(numRows_item)
  }

  @inline private[format] def readRowGroupsValue(_iprot: TProtocol): Seq[parquet.format.RowGroup] = {
    val _list = _iprot.readListBegin()
    if (_list.size == 0) {
      _iprot.readListEnd()
      Nil
    } else {
      val _rv = new mutable$ArrayBuffer[parquet.format.RowGroup](_list.size)
      var _i = 0
      while (_i < _list.size) {
        _rv += {
          parquet.format.RowGroup.decode(_iprot)
        }
        _i += 1
      }
      _iprot.readListEnd()
      _rv
    }
  }

  @inline private def writeRowGroupsField(rowGroups_item: Seq[parquet.format.RowGroup], _oprot: TProtocol): Unit = {
    _oprot.writeFieldBegin(RowGroupsField)
    writeRowGroupsValue(rowGroups_item, _oprot)
    _oprot.writeFieldEnd()
  }

  @inline private def writeRowGroupsValue(rowGroups_item: Seq[parquet.format.RowGroup], _oprot: TProtocol): Unit = {
    _oprot.writeListBegin(new TList(TType.STRUCT, rowGroups_item.size))
    rowGroups_item match {
      case _: IndexedSeq[_] =>
        var _i = 0
        val _size = rowGroups_item.size
        while (_i < _size) {
          val rowGroups_item_element = rowGroups_item(_i)
          rowGroups_item_element.write(_oprot)
          _i += 1
        }
      case _ =>
        rowGroups_item.foreach { rowGroups_item_element =>
          rowGroups_item_element.write(_oprot)
        }
    }
    _oprot.writeListEnd()
  }

  @inline private[format] def readKeyValueMetadataValue(_iprot: TProtocol): Seq[parquet.format.KeyValue] = {
    val _list = _iprot.readListBegin()
    if (_list.size == 0) {
      _iprot.readListEnd()
      Nil
    } else {
      val _rv = new mutable$ArrayBuffer[parquet.format.KeyValue](_list.size)
      var _i = 0
      while (_i < _list.size) {
        _rv += {
          parquet.format.KeyValue.decode(_iprot)
        }
        _i += 1
      }
      _iprot.readListEnd()
      _rv
    }
  }

  @inline private def writeKeyValueMetadataField(keyValueMetadata_item: Seq[parquet.format.KeyValue], _oprot: TProtocol): Unit = {
    _oprot.writeFieldBegin(KeyValueMetadataField)
    writeKeyValueMetadataValue(keyValueMetadata_item, _oprot)
    _oprot.writeFieldEnd()
  }

  @inline private def writeKeyValueMetadataValue(keyValueMetadata_item: Seq[parquet.format.KeyValue], _oprot: TProtocol): Unit = {
    _oprot.writeListBegin(new TList(TType.STRUCT, keyValueMetadata_item.size))
    keyValueMetadata_item match {
      case _: IndexedSeq[_] =>
        var _i = 0
        val _size = keyValueMetadata_item.size
        while (_i < _size) {
          val keyValueMetadata_item_element = keyValueMetadata_item(_i)
          keyValueMetadata_item_element.write(_oprot)
          _i += 1
        }
      case _ =>
        keyValueMetadata_item.foreach { keyValueMetadata_item_element =>
          keyValueMetadata_item_element.write(_oprot)
        }
    }
    _oprot.writeListEnd()
  }

  @inline private[format] def readCreatedByValue(_iprot: TProtocol): String = {
    _iprot.readString()
  }

  @inline private def writeCreatedByField(createdBy_item: String, _oprot: TProtocol): Unit = {
    _oprot.writeFieldBegin(CreatedByField)
    writeCreatedByValue(createdBy_item, _oprot)
    _oprot.writeFieldEnd()
  }

  @inline private def writeCreatedByValue(createdBy_item: String, _oprot: TProtocol): Unit = {
    _oprot.writeString(createdBy_item)
  }


  object Immutable extends ThriftStructCodec3[FileMetaData] {
    override def encode(_item: FileMetaData, _oproto: TProtocol): Unit = { _item.write(_oproto) }
    override def decode(_iprot: TProtocol): FileMetaData = FileMetaData.decode(_iprot)
    override lazy val metaData: ThriftStructMetaData[FileMetaData] = FileMetaData.metaData
  }

  /**
   * The default read-only implementation of FileMetaData.  You typically should not need to
   * directly reference this class; instead, use the FileMetaData.apply method to construct
   * new instances.
   */
  class Immutable(
      val version: Int,
      val schema: Seq[parquet.format.SchemaElement],
      val numRows: Long,
      val rowGroups: Seq[parquet.format.RowGroup],
      val keyValueMetadata: _root_.scala.Option[Seq[parquet.format.KeyValue]],
      val createdBy: _root_.scala.Option[String],
      override val _passthroughFields: immutable$Map[Short, TFieldBlob])
    extends FileMetaData {
    def this(
      version: Int,
      schema: Seq[parquet.format.SchemaElement] = Seq[parquet.format.SchemaElement](),
      numRows: Long,
      rowGroups: Seq[parquet.format.RowGroup] = Seq[parquet.format.RowGroup](),
      keyValueMetadata: _root_.scala.Option[Seq[parquet.format.KeyValue]] = _root_.scala.None,
      createdBy: _root_.scala.Option[String] = _root_.scala.None
    ) = this(
      version,
      schema,
      numRows,
      rowGroups,
      keyValueMetadata,
      createdBy,
      Map.empty
    )
  }

  /**
   * This is another Immutable, this however keeps strings as lazy values that are lazily decoded from the backing
   * array byte on read.
   */
  private[this] class LazyImmutable(
      _proto: LazyTProtocol,
      _buf: Array[Byte],
      _start_offset: Int,
      _end_offset: Int,
      val version: Int,
      val schema: Seq[parquet.format.SchemaElement],
      val numRows: Long,
      val rowGroups: Seq[parquet.format.RowGroup],
      val keyValueMetadata: _root_.scala.Option[Seq[parquet.format.KeyValue]],
      created_byOffset: Int,
      override val _passthroughFields: immutable$Map[Short, TFieldBlob])
    extends FileMetaData {

    override def write(_oprot: TProtocol): Unit = {
      _oprot match {
        case i: LazyTProtocol => i.writeRaw(_buf, _start_offset, _end_offset - _start_offset)
        case _ => super.write(_oprot)
      }
    }

    lazy val createdBy: _root_.scala.Option[String] =
      if (created_byOffset == -1)
        None
      else {
        Some(_proto.decodeString(_buf, created_byOffset))
      }

    /**
     * Override the super hash code to make it a lazy val rather than def.
     *
     * Calculating the hash code can be expensive, caching it where possible
     * can provide significant performance wins. (Key in a hash map for instance)
     * Usually not safe since the normal constructor will accept a mutable map or
     * set as an arg
     * Here however we control how the class is generated from serialized data.
     * With the class private and the contract that we throw away our mutable references
     * having the hash code lazy here is safe.
     */
    override lazy val hashCode = super.hashCode
  }

  /**
   * This Proxy trait allows you to extend the FileMetaData trait with additional state or
   * behavior and implement the read-only methods from FileMetaData using an underlying
   * instance.
   */
  trait Proxy extends FileMetaData {
    protected def _underlying_FileMetaData: FileMetaData
    override def version: Int = _underlying_FileMetaData.version
    override def schema: Seq[parquet.format.SchemaElement] = _underlying_FileMetaData.schema
    override def numRows: Long = _underlying_FileMetaData.numRows
    override def rowGroups: Seq[parquet.format.RowGroup] = _underlying_FileMetaData.rowGroups
    override def keyValueMetadata: _root_.scala.Option[Seq[parquet.format.KeyValue]] = _underlying_FileMetaData.keyValueMetadata
    override def createdBy: _root_.scala.Option[String] = _underlying_FileMetaData.createdBy
    override def _passthroughFields = _underlying_FileMetaData._passthroughFields
  }
}

/**
 * Prefer the companion object's [[parquet.format.FileMetaData.apply]]
 * for construction if you don't need to specify passthrough fields.
 */
trait FileMetaData
  extends ThriftStruct
  with _root_.scala.Product6[Int, Seq[parquet.format.SchemaElement], Long, Seq[parquet.format.RowGroup], Option[Seq[parquet.format.KeyValue]], Option[String]]
  with HasThriftStructCodec3[FileMetaData]
  with java.io.Serializable
{
  import FileMetaData._

  def version: Int
  def schema: Seq[parquet.format.SchemaElement]
  def numRows: Long
  def rowGroups: Seq[parquet.format.RowGroup]
  def keyValueMetadata: _root_.scala.Option[Seq[parquet.format.KeyValue]]
  def createdBy: _root_.scala.Option[String]

  def _passthroughFields: immutable$Map[Short, TFieldBlob] = immutable$Map.empty

  def _1 = version
  def _2 = schema
  def _3 = numRows
  def _4 = rowGroups
  def _5 = keyValueMetadata
  def _6 = createdBy

  def toTuple: _root_.scala.Tuple6[Int, Seq[parquet.format.SchemaElement], Long, Seq[parquet.format.RowGroup], Option[Seq[parquet.format.KeyValue]], Option[String]] = {
    (
      version,
      schema,
      numRows,
      rowGroups,
      keyValueMetadata,
      createdBy
    )
  }


  /**
   * Gets a field value encoded as a binary blob using TCompactProtocol.  If the specified field
   * is present in the passthrough map, that value is returned.  Otherwise, if the specified field
   * is known and not optional and set to None, then the field is serialized and returned.
   */
  def getFieldBlob(_fieldId: Short): _root_.scala.Option[TFieldBlob] = {
    lazy val _buff = new TMemoryBuffer(32)
    lazy val _oprot = new TCompactProtocol(_buff)
    _passthroughFields.get(_fieldId) match {
      case blob: _root_.scala.Some[TFieldBlob] => blob
      case _root_.scala.None => {
        val _fieldOpt: _root_.scala.Option[TField] =
          _fieldId match {
            case 1 =>
              if (true) {
                writeVersionValue(version, _oprot)
                _root_.scala.Some(FileMetaData.VersionField)
              } else {
                _root_.scala.None
              }
            case 2 =>
              if (schema ne null) {
                writeSchemaValue(schema, _oprot)
                _root_.scala.Some(FileMetaData.SchemaField)
              } else {
                _root_.scala.None
              }
            case 3 =>
              if (true) {
                writeNumRowsValue(numRows, _oprot)
                _root_.scala.Some(FileMetaData.NumRowsField)
              } else {
                _root_.scala.None
              }
            case 4 =>
              if (rowGroups ne null) {
                writeRowGroupsValue(rowGroups, _oprot)
                _root_.scala.Some(FileMetaData.RowGroupsField)
              } else {
                _root_.scala.None
              }
            case 5 =>
              if (keyValueMetadata.isDefined) {
                writeKeyValueMetadataValue(keyValueMetadata.get, _oprot)
                _root_.scala.Some(FileMetaData.KeyValueMetadataField)
              } else {
                _root_.scala.None
              }
            case 6 =>
              if (createdBy.isDefined) {
                writeCreatedByValue(createdBy.get, _oprot)
                _root_.scala.Some(FileMetaData.CreatedByField)
              } else {
                _root_.scala.None
              }
            case _ => _root_.scala.None
          }
        _fieldOpt match {
          case _root_.scala.Some(_field) =>
            _root_.scala.Some(TFieldBlob(_field, Buf.ByteArray.Owned(_buff.getArray())))
          case _root_.scala.None =>
            _root_.scala.None
        }
      }
    }
  }

  /**
   * Collects TCompactProtocol-encoded field values according to `getFieldBlob` into a map.
   */
  def getFieldBlobs(ids: TraversableOnce[Short]): immutable$Map[Short, TFieldBlob] =
    (ids flatMap { id => getFieldBlob(id) map { id -> _ } }).toMap

  /**
   * Sets a field using a TCompactProtocol-encoded binary blob.  If the field is a known
   * field, the blob is decoded and the field is set to the decoded value.  If the field
   * is unknown and passthrough fields are enabled, then the blob will be stored in
   * _passthroughFields.
   */
  def setField(_blob: TFieldBlob): FileMetaData = {
    var version: Int = this.version
    var schema: Seq[parquet.format.SchemaElement] = this.schema
    var numRows: Long = this.numRows
    var rowGroups: Seq[parquet.format.RowGroup] = this.rowGroups
    var keyValueMetadata: _root_.scala.Option[Seq[parquet.format.KeyValue]] = this.keyValueMetadata
    var createdBy: _root_.scala.Option[String] = this.createdBy
    var _passthroughFields = this._passthroughFields
    _blob.id match {
      case 1 =>
        version = readVersionValue(_blob.read)
      case 2 =>
        schema = readSchemaValue(_blob.read)
      case 3 =>
        numRows = readNumRowsValue(_blob.read)
      case 4 =>
        rowGroups = readRowGroupsValue(_blob.read)
      case 5 =>
        keyValueMetadata = _root_.scala.Some(readKeyValueMetadataValue(_blob.read))
      case 6 =>
        createdBy = _root_.scala.Some(readCreatedByValue(_blob.read))
      case _ => _passthroughFields += (_blob.id -> _blob)
    }
    new Immutable(
      version,
      schema,
      numRows,
      rowGroups,
      keyValueMetadata,
      createdBy,
      _passthroughFields
    )
  }

  /**
   * If the specified field is optional, it is set to None.  Otherwise, if the field is
   * known, it is reverted to its default value; if the field is unknown, it is removed
   * from the passthroughFields map, if present.
   */
  def unsetField(_fieldId: Short): FileMetaData = {
    var version: Int = this.version
    var schema: Seq[parquet.format.SchemaElement] = this.schema
    var numRows: Long = this.numRows
    var rowGroups: Seq[parquet.format.RowGroup] = this.rowGroups
    var keyValueMetadata: _root_.scala.Option[Seq[parquet.format.KeyValue]] = this.keyValueMetadata
    var createdBy: _root_.scala.Option[String] = this.createdBy

    _fieldId match {
      case 1 =>
        version = 0
      case 2 =>
        schema = Seq[parquet.format.SchemaElement]()
      case 3 =>
        numRows = 0L
      case 4 =>
        rowGroups = Seq[parquet.format.RowGroup]()
      case 5 =>
        keyValueMetadata = _root_.scala.None
      case 6 =>
        createdBy = _root_.scala.None
      case _ =>
    }
    new Immutable(
      version,
      schema,
      numRows,
      rowGroups,
      keyValueMetadata,
      createdBy,
      _passthroughFields - _fieldId
    )
  }

  /**
   * If the specified field is optional, it is set to None.  Otherwise, if the field is
   * known, it is reverted to its default value; if the field is unknown, it is removed
   * from the passthroughFields map, if present.
   */
  def unsetVersion: FileMetaData = unsetField(1)

  def unsetSchema: FileMetaData = unsetField(2)

  def unsetNumRows: FileMetaData = unsetField(3)

  def unsetRowGroups: FileMetaData = unsetField(4)

  def unsetKeyValueMetadata: FileMetaData = unsetField(5)

  def unsetCreatedBy: FileMetaData = unsetField(6)


  override def write(_oprot: TProtocol): Unit = {
    FileMetaData.validate(this)
    _oprot.writeStructBegin(Struct)
    writeVersionField(version, _oprot)
    if (schema ne null) writeSchemaField(schema, _oprot)
    writeNumRowsField(numRows, _oprot)
    if (rowGroups ne null) writeRowGroupsField(rowGroups, _oprot)
    if (keyValueMetadata.isDefined) writeKeyValueMetadataField(keyValueMetadata.get, _oprot)
    if (createdBy.isDefined) writeCreatedByField(createdBy.get, _oprot)
    if (_passthroughFields.nonEmpty) {
      _passthroughFields.values.foreach { _.write(_oprot) }
    }
    _oprot.writeFieldStop()
    _oprot.writeStructEnd()
  }

  def copy(
    version: Int = this.version,
    schema: Seq[parquet.format.SchemaElement] = this.schema,
    numRows: Long = this.numRows,
    rowGroups: Seq[parquet.format.RowGroup] = this.rowGroups,
    keyValueMetadata: _root_.scala.Option[Seq[parquet.format.KeyValue]] = this.keyValueMetadata,
    createdBy: _root_.scala.Option[String] = this.createdBy,
    _passthroughFields: immutable$Map[Short, TFieldBlob] = this._passthroughFields
  ): FileMetaData =
    new Immutable(
      version,
      schema,
      numRows,
      rowGroups,
      keyValueMetadata,
      createdBy,
      _passthroughFields
    )

  override def canEqual(other: Any): Boolean = other.isInstanceOf[FileMetaData]

  private def _equals(x: FileMetaData, y: FileMetaData): Boolean =
      x.productArity == y.productArity &&
      x.productIterator.sameElements(y.productIterator)

  override def equals(other: Any): Boolean =
    canEqual(other) &&
      _equals(this, other.asInstanceOf[FileMetaData]) &&
      _passthroughFields == other.asInstanceOf[FileMetaData]._passthroughFields

  override def hashCode: Int = _root_.scala.runtime.ScalaRunTime._hashCode(this)

  override def toString: String = _root_.scala.runtime.ScalaRunTime._toString(this)


  override def productArity: Int = 6

  override def productElement(n: Int): Any = n match {
    case 0 => this.version
    case 1 => this.schema
    case 2 => this.numRows
    case 3 => this.rowGroups
    case 4 => this.keyValueMetadata
    case 5 => this.createdBy
    case _ => throw new IndexOutOfBoundsException(n.toString)
  }

  override def productPrefix: String = "FileMetaData"

  def _codec: ThriftStructCodec3[FileMetaData] = FileMetaData
}

